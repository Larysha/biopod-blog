[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "A collection of free, open-source tools and resources I’ve found useful over the years. Some I’ve worked through extensively, others I’m still exploring. If it’s here, it’s worth your time."
  },
  {
    "objectID": "resources.html#machine-learning-deep-learning",
    "href": "resources.html#machine-learning-deep-learning",
    "title": "Resources",
    "section": "Machine Learning & Deep Learning",
    "text": "Machine Learning & Deep Learning\nDeep Learning with R for Beginners – Hands-on introduction to deep learning using R.\nNeural Networks and Deep Learning – Free online book that builds intuition from the ground up.\nBut what is a neural network? – Honestly the best and most intuitive explanation of how neural networks work I’ve heard.\nTowards Data Science – Fantastic blog covering a broad range of topics in data science and ML.\nDeep Learning with R (2nd Edition) – A very underrated book that’s well-written and accessible. Free to read at this link.\nMachine Learning A-Z: Tips and Resources – Curated collection of ML learning materials."
  },
  {
    "objectID": "resources.html#blogs-portfolios-worth-following",
    "href": "resources.html#blogs-portfolios-worth-following",
    "title": "Resources",
    "section": "Blogs & Portfolios Worth Following",
    "text": "Blogs & Portfolios Worth Following\nTom Jenkins – Clean portfolio with solid bioinformatics content.\nDave Tang’s Blog – Long-running blog with practical bioinformatics tutorials and musings.\nDavid Goodsell’s Molecular Illustrations – Making science come alive with art. An utter inspiration.\nDiving into Genetics and Genomics – I’ve been following Tommy for over seven years—he was one of the first bioinformaticians I found online. Produces an insane amount of content on a wide range of topics."
  },
  {
    "objectID": "resources.html#programming-bioinformatics",
    "href": "resources.html#programming-bioinformatics",
    "title": "Resources",
    "section": "Programming & Bioinformatics",
    "text": "Programming & Bioinformatics\nSIB Swiss Institute of Bioinformatics Training Materials – High-quality courses and tutorials.\nLibreTexts: Computational Biology – Free textbook covering core concepts.\nBioinformatics Workbook – Comprehensive resource covering data analysis workflows.\nComputational Genomics with R – Builds from basic biology through to multi-omics. Excellent progression.\nCoddy – Interactive coding tutorials.\nAutomate the Boring Stuff with Python – A popular book that makes learning Python feel useful and practical. Free to read online.\nRosalind – Practice problems for brushing up on bioinformatics and programming concepts.\nHands-On Programming with R – Nice introduction to R.\nHadley Wickham – The legend of the tidyverse. Books available free online, including: - Tidy Modeling with R\nMIT Computational Biology Course Materials – Free MIT course content in computational biology taught by Manolis Kellis."
  },
  {
    "objectID": "resources.html#grapevine-resources",
    "href": "resources.html#grapevine-resources",
    "title": "Resources",
    "section": "Grapevine Resources",
    "text": "Grapevine Resources\nA little niche—for those looking for up-to-date community tools.\nGrapedia – Comprehensive grapevine genomics and genetics database."
  },
  {
    "objectID": "resources.html#useful-tools-repositories",
    "href": "resources.html#useful-tools-repositories",
    "title": "Resources",
    "section": "Useful Tools & Repositories",
    "text": "Useful Tools & Repositories\nOur World in Data – Research and data on global challenges. Invaluable for context.\ncsvtk – Command-line toolkit for working with CSV files.\nBioNumbers – The cell biology by the numbers book. Fascinating reference.\nFriends Don’t Let Friends Make Bad Graphs – A guide to data visualisation best practices.\nAcademic Workflow: Zotero + Obsidian – This post became my key guide for setting up a writing system I now can’t imagine functioning without. The ideal organisational backbone for research and writing.\nPLOS Ten Simple Rules Collection – A surprisingly helpful and practical collection of tips for academics.\nSticky by YesChat AI – In case your drawing skills don’t extend to stick figures."
  },
  {
    "objectID": "resources.html#ai-tools-for-literature-management",
    "href": "resources.html#ai-tools-for-literature-management",
    "title": "Resources",
    "section": "AI Tools for Literature Management",
    "text": "AI Tools for Literature Management\nResearch Rabbit – Visual citation networks and paper discovery.\nAnara AI – AI-powered literature review assistant.\nSciSpace – Simplifies reading and understanding research papers.\nNotebookLM – Google’s AI notebook for synthesising research.\nConsensus - AI-powered search engine\nThere are honestly too many of these now, but these are the ones I’ve found useful in “free” tier for gathering, organising and querying papers."
  },
  {
    "objectID": "resources.html#because-its-important-to-have-hobbies",
    "href": "resources.html#because-its-important-to-have-hobbies",
    "title": "Resources",
    "section": "Because It’s Important to Have Hobbies",
    "text": "Because It’s Important to Have Hobbies\nMarty Music – Guitar tutorials for absolute beginners.\nJustinGuitar – Structured, free guitar course.\nDrawabox – Learn to draw through deliberate practice.\nFree Learning List – Massive collection of free online courses across all subjects."
  },
  {
    "objectID": "resources.html#youtube-channels",
    "href": "resources.html#youtube-channels",
    "title": "Resources",
    "section": "YouTube Channels",
    "text": "YouTube Channels\n\nLearning & Explainers\n3Blue1Brown – Incredible maths visualisations and explanations.\nPrimer – Simulations of evolution and game theory. Also very cute.\nfreeCodeCamp – Full programming courses for free.\nAlex The Analyst – Data analysis and career advice.\nUp and Atom – Physics made approachable.\nDomain of Science – Maps of scientific knowledge.\n\n\nThought-Provoking Content\nYuval Noah Harari: A Short Chat – On technology, humanity, and attention.\nGlobal Birth Rates – Context on demographic shifts.\n\n\nFor procrastinating…\nHow to make a carrot whistle – Peak internet.\nGetting Started in Something Creative – Practical advice for when you don’t know where to start with drawing. This guy’s entire channel is just great fun.\nChinggis khaanii Magtaal - Batzorig Vaanchig - I can’t stop listening to this..\nThe Promised Land Series – If the Old Testament was told “Office”-style.\nFoil Arms and Hog – Irish comedy sketch group. Innocent humour for when you need a laugh."
  },
  {
    "objectID": "posts/wsl_setup/index.html",
    "href": "posts/wsl_setup/index.html",
    "title": "WSL for Bioinformatics: Setting Up a Functional (and Pretty) Environment",
    "section": "",
    "text": "The Unix command line is the foundational tool for bioinformatics. It’s where you access servers, wrangle large files, and automate analysis by chaining together customisable commands. Most bioinformatics software is built to run in this environment because it’s simply the most efficient way to work with biological data at scale.\nIf you’re on Linux or macOS, you’re sorted—the terminal is already there. But Windows users have historically been stuck. PowerShell works fundamentally differently to Unix shells, leaving you with awkward workarounds: dual-booting, virtual machines, or third-party tools like Cygwin. None of these provide a smooth experience.\nEnter the Windows Subsystem for Linux (WSL), first released in 2016. WSL2 offers a proper Linux kernel running directly on Windows—no virtual machine overhead, full access to both file systems, and seamless integration. For bioinformatics, you’ll want Ubuntu, a Debian distribution compatible with virtually all third-party tools."
  },
  {
    "objectID": "posts/wsl_setup/index.html#why-wsl",
    "href": "posts/wsl_setup/index.html#why-wsl",
    "title": "WSL for Bioinformatics: Setting Up a Functional (and Pretty) Environment",
    "section": "",
    "text": "The Unix command line is the foundational tool for bioinformatics. It’s where you access servers, wrangle large files, and automate analysis by chaining together customisable commands. Most bioinformatics software is built to run in this environment because it’s simply the most efficient way to work with biological data at scale.\nIf you’re on Linux or macOS, you’re sorted—the terminal is already there. But Windows users have historically been stuck. PowerShell works fundamentally differently to Unix shells, leaving you with awkward workarounds: dual-booting, virtual machines, or third-party tools like Cygwin. None of these provide a smooth experience.\nEnter the Windows Subsystem for Linux (WSL), first released in 2016. WSL2 offers a proper Linux kernel running directly on Windows—no virtual machine overhead, full access to both file systems, and seamless integration. For bioinformatics, you’ll want Ubuntu, a Debian distribution compatible with virtually all third-party tools."
  },
  {
    "objectID": "posts/wsl_setup/index.html#installing-wsl",
    "href": "posts/wsl_setup/index.html#installing-wsl",
    "title": "WSL for Bioinformatics: Setting Up a Functional (and Pretty) Environment",
    "section": "Installing WSL",
    "text": "Installing WSL\nRequirements: Windows 10 (build 19041+) or Windows 11\n\nOpen Command Prompt as administrator (Win Key → search “command prompt” → “run as administrator”)\nType: wsl.exe --install\nRestart your computer when prompted\n\nThat’s it. Ubuntu installs by default.\nWhen you first open Ubuntu: - Create a username and password (doesn’t need to match your Windows credentials) - Remember this password—you’ll need it for any sudo commands - Note: you won’t see characters when typing passwords in terminal. This is normal.\nUpdate your system:\nsudo apt update\nsudo apt full-upgrade\nYour WSL is now set up."
  },
  {
    "objectID": "posts/wsl_setup/index.html#terminal-options",
    "href": "posts/wsl_setup/index.html#terminal-options",
    "title": "WSL for Bioinformatics: Setting Up a Functional (and Pretty) Environment",
    "section": "Terminal Options",
    "text": "Terminal Options\nWSL runs from the Ubuntu terminal by default, which works perfectly fine. But if you want a more polished experience with multiple tabs and better aesthetics, consider:\nWindows Terminal — Microsoft’s modern terminal app, available from the Microsoft Store. Supports multiple tabs, custom themes, and sets WSL as the default profile easily.\nFluent Terminal (recommended for the aethetics)  — An alternative with similar features, available on GitHub or the MS Store. Open Menu → New Tab → WSL, then set it as default in Settings → Profiles.\nBoth let you run multiple shells (WSL, PowerShell, Command Prompt) in one window.\n\nMaking It Pretty (Optional)\nThis won’t affect functionality, but if you want your terminal to look good:\nEasy option: Use the built-in themes in Windows Terminal (Settings → Appearance) or Fluent Terminal (Settings → Themes).\nCustomisation: Edit your ~/.bashrc file to change your prompt colours and format. Modify the PS1= variable—this guide covers the details.\nPre-built themes: Install oh-my-bash for ready-made themes. Note: installation creates a new .bashrc file. Your old one gets backed up as .bashrc.omb-backup-[date]. Copy any custom aliases or settings from the backup to the new file. Browse themes here and set your choice by editing the OSH_THEME variable in .bashrc."
  },
  {
    "objectID": "posts/wsl_setup/index.html#navigating-between-windows-and-linux",
    "href": "posts/wsl_setup/index.html#navigating-between-windows-and-linux",
    "title": "WSL for Bioinformatics: Setting Up a Functional (and Pretty) Environment",
    "section": "Navigating Between Windows and Linux",
    "text": "Navigating Between Windows and Linux\nYour WSL home directory lives at /home/your_username/, separate from your Windows files. To access Windows files from WSL, mount your C: drive:\ncd /mnt/c/Users/your_windows_username/\nAll Windows drives are accessible under /mnt/. External drives usually appear automatically as /mnt/d/, /mnt/e/, etc.\nIf an external drive doesn’t mount automatically:\nsudo fdisk -l  # list all partitions\nsudo mount -t drvfs d: /mnt/d  # mount drive d:\nShortcuts: Create aliases in ~/.bashrc to jump to frequently used directories:\nnano ~/.bashrc\n# Add at the end:\nalias docs=\"cd /mnt/c/Users/your_name/Documents\"\n# Save, then: source ~/.bashrc\nNow typing docs takes you straight there.\nKey points: - Access Windows from WSL: /mnt/c/ - The leading / is essential (represents root) - Hidden files (starting with .) appear with ls -a"
  },
  {
    "objectID": "posts/wsl_setup/index.html#adding-programs-to-path",
    "href": "posts/wsl_setup/index.html#adding-programs-to-path",
    "title": "WSL for Bioinformatics: Setting Up a Functional (and Pretty) Environment",
    "section": "Adding Programs to PATH",
    "text": "Adding Programs to PATH\nWhen you install third-party software in your home directory (~/), you’ll want it accessible from any working directory. Add the executable’s location to your $PATH variable:\nnano ~/.bashrc\n# Add at the end (replace paths with your software locations):\nexport PATH=\"/usr/local/sbin:/home/bin/your/software:$PATH\"\n# Save and reload:\nsource ~/.bashrc\n\n# Verify:\necho $PATH\nwhich your_program  # shows where the executable is located\nOnce in PATH, your software works from anywhere—including Windows directories."
  },
  {
    "objectID": "posts/wsl_setup/index.html#essential-tools",
    "href": "posts/wsl_setup/index.html#essential-tools",
    "title": "WSL for Bioinformatics: Setting Up a Functional (and Pretty) Environment",
    "section": "Essential Tools",
    "text": "Essential Tools\n\nConda\nMiniconda provides Python package management and virtual environments:\ncd ~\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh\nbash ~/miniconda.sh -b -u -p ~/miniconda3\nrm ~/miniconda.sh\n~/miniconda3/bin/conda init bash\nRestart your terminal. You should see (base) before your prompt, indicating the base conda environment is active. Learn conda basics here.\nAlternative: If you prefer pip and venv:\nsudo apt update && sudo apt upgrade\nsudo apt install python3 python3-pip python3-venv ipython3\n\n\nGit\nGit comes pre-installed with WSL2. Configure it:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"  # use your GitHub email\ngit config --global core.editor \"nano\"  # or \"code\" for VSCode\nCheck your settings: git config --list\n\n\nR and Devtools\nsudo apt update\nsudo apt install r-base\n\n# Optional but useful R dependencies:\nsudo apt install \\\n  r-base-core \\\n  r-recommended \\\n  r-base-dev \\\n  build-essential \\\n  libcurl4-gnutls-dev \\\n  libxml2-dev \\\n  libssl-dev\nLaunch R by typing R, then install devtools:\ninstall.packages(\"devtools\")\nExit R with q() and n.\n\n\nDocker\nDocker containers eliminate dependency headaches by packaging everything an application needs to run. Many bioinformatics tools are available as Docker images.\n\nDownload Docker Desktop for Windows\nDuring installation, enable WSL2 integration\nOpen Docker Desktop → Settings → General → ensure “Use the WSL 2 based engine” is enabled\n\nVerify in terminal: docker --version\nNote: Docker only works in WSL whilst Docker Desktop is running in Windows.\nFull setup guide: Docker WSL documentation\n\n\nNextflow\nNextflow creates scalable, reproducible bioinformatics pipelines. Setup is straightforward:\n# Check Java version (needs v11+):\njava --version\n\n# If you need to install Java:\ncurl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nsdk install java 17.0.10-tem\n\n# Install Nextflow:\nwget -qO- https://get.nextflow.io | bash\nchmod +x nextflow\nsudo mv nextflow /usr/local/bin/  # makes it globally accessible\nExplore community pipelines at nf-core.\n\n\nVSCode Integration\nInstall VSCode on Windows, then add the Remote Development Extension Pack.\nFrom WSL, open any directory in VSCode:\ncode .\nVSCode will connect to WSL automatically and you can edit files, run terminals, and debug directly through the WSL environment.\n\nYou now have a fully functional bioinformatics environment: Unix tools running natively on Windows, access to both file systems, and all the major software frameworks installed. Time to start analysing data."
  },
  {
    "objectID": "notes/theme_guide.html",
    "href": "notes/theme_guide.html",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "rish-blogs/\n├── _quarto.yml                 # Site configuration\n├── custom-theme.scss           # Your custom theme (THE MAIN FILE)\n├── fonts/\n│   └── apricots.woff2          # Custom Apricots font\n├── images/\n│   ├── favicon.png\n│   ├── profile.png\n│   ├── hero-collage.png\n├── index.qmd                   # Home page\n├── about.qmd                   # About page\n├── posts.qmd                   # Blog listing\n├── resources.qmd               # Resources page\n└── posts/                      # Blog posts directory\n\n\n\n\nThe theme file has two main sections:\n\n\nThis is where you control everything about your design:\n// Colors\n$bg-primary: #e0cdc5;          // Home/Blog background\n$bg-secondary: #d5c3b3;        // About/Resources background\n$text-heading: #323619;        // Heading color\n$text-body: #000000;           // Body text color\n\n// Fonts\n$font-family-sans-serif: 'Crimson Text', serif;  // Body font\n$headings-font-family: 'Crimson Text', serif;     // Headings font\n\n// Spacing (THIS IS THE KEY!)\n$spacer: 1rem;                    // Base spacing unit\n$paragraph-margin-bottom: 1rem;   // Space after paragraphs\n$headings-margin-bottom: 0.5rem;  // Space after headings\n\n// Font sizes\n$h1-font-size: 2.5rem;\n$h2-font-size: 2rem;\n// ... etc\n\n\n\nThis is where you write actual CSS rules that use the variables above.\n\n\n\n\n\n\nEdit the color variables in the /*-- scss:defaults --*/ section:\n$bg-primary: #your-hex-color;\n$text-heading: #your-hex-color;\n\n\n\nThe theme includes spacing control classes:\n\n.tight-spacing - Slightly reduced spacing (margin-bottom: 0.5rem)\n.very-tight-spacing - Very tight spacing (margin-bottom: 0.1rem, line-height: 1)\n\nUse these in your .qmd files:\n&lt;h1 class=\"very-tight-spacing\"&gt;Title&lt;/h1&gt;\n&lt;p&gt;Subtitle right below&lt;/p&gt;\n\n\n\nAdjust heading sizes in the defaults section:\n$h1-font-size: 3rem;    // Make h1 bigger\n$h2-font-size: 2.5rem;  // Make h2 bigger\n\n\n\nFind the relevant section in /*-- scss:rules --*/:\n.about-entity .about-image img {\n  max-width: 300px;  // Change this value\n}\n\n.hero-image-container {\n  flex: 0 0 45%;      // Change percentage for hero image width\n  max-width: 500px;   // Change max width\n}\n\n\n\nIf you want additional accent colors:\n// In defaults section:\n$accent-primary: #your-color;\n$accent-secondary: #another-color;\n\n// Then use them in rules section:\n.special-element {\n  background-color: $accent-primary;\n  border-color: $accent-secondary;\n}\n\n\n\n\n\n\nThe custom Apricots font is loaded like this:\n@font-face {\n  font-family: 'Apricots';\n  src: url('fonts/apricots.woff2') format('woff2');\n  font-weight: normal;\n  font-style: normal;\n  font-display: swap;\n}\nImportant: The fonts/ directory is listed as a resource in _quarto.yml so Quarto copies it to the output directory.\n\n\n\n\nGet the .woff2 file\nPut it in the fonts/ directory\nMake sure _quarto.yml has fonts/ listed as a resource (already done)\nAdd this to the top of custom-theme.scss:\n\n@font-face {\n  font-family: 'YourFontName';\n  src: url('fonts/your-font-file.woff2') format('woff2');\n  font-weight: normal;\n  font-style: normal;\n  font-display: swap;\n}\n\nUse it in your theme:\n\n$font-family-sans-serif: 'YourFontName', sans-serif;\n\n\n\n\n\n\n.about-entity .about-image img {\n  border-radius: 50%;  // Circular\n  // border-radius: 10px;  // Slightly rounded\n  // border-radius: 0;  // Square\n}\n\n\n\n.container,\n.container-fluid {\n  max-width: 1400px;  // Change this value\n}\n\n\n\n.hero-image-container {\n  flex: 0 0 45%;  // Change 45% to adjust image width\n  // 30% = smaller image, more text space\n  // 60% = bigger image, less text space\n}\n\n\n\n#recent-posts .quarto-listing-grid .list.grid {\n  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n  //                                        ↑ Change this value\n  // 300px = bigger cards\n  // 200px = smaller cards, more columns\n  gap: 1.5rem;  // Space between cards\n}\n\n\n\n\n\n\nDO NOT DELETE IT! This is where Quarto builds your website.\n\ndocs/ = Your compiled, ready-to-deploy website\nGitHub Pages serves your site FROM this folder\nContains: HTML files, CSS, images, all site assets\n\nShould it be in .gitignore? - ❌ NO for GitHub Pages deployment - ✅ YES if using GitHub Actions to build (it rebuilds on every push)\nYour setup: You’re using GitHub Actions, so: 1. The .gitignore excludes docs/ from version control 2. GitHub Actions builds the site on each push 3. The action deploys the built site automatically\nIf you want to commit docs/ (alternative approach): 1. Remove docs/ from .gitignore 2. Commit the built site: git add docs/ 3. GitHub Pages serves directly from docs/ folder 4. No need for GitHub Actions\n\n\n\ncd /home/rish/rish-blogs\nquarto preview\n\n\n\nquarto render\nQuarto will automatically: 1. Compile your .scss file into CSS 2. Generate all your pages 3. Output to docs/ directory\n\n\n\ngit add .\ngit commit -m \"Updated custom theme\"\ngit push origin main\nGitHub Actions will automatically deploy your site.\n\n\n\n\nThe theme includes mobile breakpoints at 768px. To customize:\n@media (max-width: 768px) {\n  // Your mobile styles here\n  h1 {\n    font-size: 2rem;  // Smaller on mobile\n  }\n}\n\n// Add additional breakpoints:\n@media (max-width: 1200px) {\n  // Tablet styles\n}\n\n@media (max-width: 480px) {\n  // Small mobile styles\n}\n\n\n\nCurrently applied to: - Main site title “The BioPod” (home page) - Section headings like “Lately…” - Navbar brand (if you add one)\nTo apply elsewhere, use:\n&lt;h1 style=\"font-family: 'Apricots', cursive;\"&gt;Your Title&lt;/h1&gt;\nOr add a class in the theme:\n.apricots-heading {\n  font-family: 'Apricots', cursive;\n}\nThen use: &lt;h1 class=\"apricots-heading\"&gt;Title&lt;/h1&gt;\n\n\n\n\n\n\nCheck the font file exists at: fonts/apricots.woff2\nVerify _quarto.yml has fonts/ listed under project: resources:\nThe path in @font-face should be: url('fonts/apricots.woff2')\nClear your browser cache (Ctrl+Shift+R)\nRe-run quarto preview\n\n\n\n\n\nMake sure you edited the $variable in the defaults section\nClear your browser cache (Ctrl+Shift+R)\nRe-run quarto preview\n\n\n\n\n\nCheck if inline styles are overriding theme styles\nUse .very-tight-spacing class for tight spacing\nAdjust $spacer variable for global spacing changes\n\n\n\n\n\nVerify _quarto.yml has: theme: custom-theme.scss\nCheck for SCSS syntax errors\nRun quarto render to see error messages\n\n\n\n\n\n\nUse Variables: Define colors/spacing as variables at the top, use them throughout\nMobile First: Test on mobile as you build\nSpacing Classes: Use .tight-spacing and .very-tight-spacing liberally\nComments: Add comments in your SCSS to remember what things do\nVersion Control: Commit changes incrementally so you can roll back if needed\n\n\n\n\n\nQuarto Themes: https://quarto.org/docs/output-formats/html-themes.html\nSass Basics: https://sass-lang.com/guide\nBootstrap Icons: https://icons.getbootstrap.com/\nFont Formats: https://web.dev/learn/design/web-fonts/\n\n\nNeed Help? Check the Quarto docs or edit custom-theme.scss directly. The file has comments explaining each section."
  },
  {
    "objectID": "notes/theme_guide.html#file-structure",
    "href": "notes/theme_guide.html#file-structure",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "rish-blogs/\n├── _quarto.yml                 # Site configuration\n├── custom-theme.scss           # Your custom theme (THE MAIN FILE)\n├── fonts/\n│   └── apricots.woff2          # Custom Apricots font\n├── images/\n│   ├── favicon.png\n│   ├── profile.png\n│   ├── hero-collage.png\n├── index.qmd                   # Home page\n├── about.qmd                   # About page\n├── posts.qmd                   # Blog listing\n├── resources.qmd               # Resources page\n└── posts/                      # Blog posts directory"
  },
  {
    "objectID": "notes/theme_guide.html#your-custom-theme-custom-theme.scss",
    "href": "notes/theme_guide.html#your-custom-theme-custom-theme.scss",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "The theme file has two main sections:\n\n\nThis is where you control everything about your design:\n// Colors\n$bg-primary: #e0cdc5;          // Home/Blog background\n$bg-secondary: #d5c3b3;        // About/Resources background\n$text-heading: #323619;        // Heading color\n$text-body: #000000;           // Body text color\n\n// Fonts\n$font-family-sans-serif: 'Crimson Text', serif;  // Body font\n$headings-font-family: 'Crimson Text', serif;     // Headings font\n\n// Spacing (THIS IS THE KEY!)\n$spacer: 1rem;                    // Base spacing unit\n$paragraph-margin-bottom: 1rem;   // Space after paragraphs\n$headings-margin-bottom: 0.5rem;  // Space after headings\n\n// Font sizes\n$h1-font-size: 2.5rem;\n$h2-font-size: 2rem;\n// ... etc\n\n\n\nThis is where you write actual CSS rules that use the variables above."
  },
  {
    "objectID": "notes/theme_guide.html#how-to-customize",
    "href": "notes/theme_guide.html#how-to-customize",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "Edit the color variables in the /*-- scss:defaults --*/ section:\n$bg-primary: #your-hex-color;\n$text-heading: #your-hex-color;\n\n\n\nThe theme includes spacing control classes:\n\n.tight-spacing - Slightly reduced spacing (margin-bottom: 0.5rem)\n.very-tight-spacing - Very tight spacing (margin-bottom: 0.1rem, line-height: 1)\n\nUse these in your .qmd files:\n&lt;h1 class=\"very-tight-spacing\"&gt;Title&lt;/h1&gt;\n&lt;p&gt;Subtitle right below&lt;/p&gt;\n\n\n\nAdjust heading sizes in the defaults section:\n$h1-font-size: 3rem;    // Make h1 bigger\n$h2-font-size: 2.5rem;  // Make h2 bigger\n\n\n\nFind the relevant section in /*-- scss:rules --*/:\n.about-entity .about-image img {\n  max-width: 300px;  // Change this value\n}\n\n.hero-image-container {\n  flex: 0 0 45%;      // Change percentage for hero image width\n  max-width: 500px;   // Change max width\n}\n\n\n\nIf you want additional accent colors:\n// In defaults section:\n$accent-primary: #your-color;\n$accent-secondary: #another-color;\n\n// Then use them in rules section:\n.special-element {\n  background-color: $accent-primary;\n  border-color: $accent-secondary;\n}"
  },
  {
    "objectID": "notes/theme_guide.html#font-setup",
    "href": "notes/theme_guide.html#font-setup",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "The custom Apricots font is loaded like this:\n@font-face {\n  font-family: 'Apricots';\n  src: url('fonts/apricots.woff2') format('woff2');\n  font-weight: normal;\n  font-style: normal;\n  font-display: swap;\n}\nImportant: The fonts/ directory is listed as a resource in _quarto.yml so Quarto copies it to the output directory.\n\n\n\n\nGet the .woff2 file\nPut it in the fonts/ directory\nMake sure _quarto.yml has fonts/ listed as a resource (already done)\nAdd this to the top of custom-theme.scss:\n\n@font-face {\n  font-family: 'YourFontName';\n  src: url('fonts/your-font-file.woff2') format('woff2');\n  font-weight: normal;\n  font-style: normal;\n  font-display: swap;\n}\n\nUse it in your theme:\n\n$font-family-sans-serif: 'YourFontName', sans-serif;"
  },
  {
    "objectID": "notes/theme_guide.html#common-customizations",
    "href": "notes/theme_guide.html#common-customizations",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": ".about-entity .about-image img {\n  border-radius: 50%;  // Circular\n  // border-radius: 10px;  // Slightly rounded\n  // border-radius: 0;  // Square\n}\n\n\n\n.container,\n.container-fluid {\n  max-width: 1400px;  // Change this value\n}\n\n\n\n.hero-image-container {\n  flex: 0 0 45%;  // Change 45% to adjust image width\n  // 30% = smaller image, more text space\n  // 60% = bigger image, less text space\n}\n\n\n\n#recent-posts .quarto-listing-grid .list.grid {\n  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n  //                                        ↑ Change this value\n  // 300px = bigger cards\n  // 200px = smaller cards, more columns\n  gap: 1.5rem;  // Space between cards\n}"
  },
  {
    "objectID": "notes/theme_guide.html#build-deploy",
    "href": "notes/theme_guide.html#build-deploy",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "DO NOT DELETE IT! This is where Quarto builds your website.\n\ndocs/ = Your compiled, ready-to-deploy website\nGitHub Pages serves your site FROM this folder\nContains: HTML files, CSS, images, all site assets\n\nShould it be in .gitignore? - ❌ NO for GitHub Pages deployment - ✅ YES if using GitHub Actions to build (it rebuilds on every push)\nYour setup: You’re using GitHub Actions, so: 1. The .gitignore excludes docs/ from version control 2. GitHub Actions builds the site on each push 3. The action deploys the built site automatically\nIf you want to commit docs/ (alternative approach): 1. Remove docs/ from .gitignore 2. Commit the built site: git add docs/ 3. GitHub Pages serves directly from docs/ folder 4. No need for GitHub Actions\n\n\n\ncd /home/rish/rish-blogs\nquarto preview\n\n\n\nquarto render\nQuarto will automatically: 1. Compile your .scss file into CSS 2. Generate all your pages 3. Output to docs/ directory\n\n\n\ngit add .\ngit commit -m \"Updated custom theme\"\ngit push origin main\nGitHub Actions will automatically deploy your site."
  },
  {
    "objectID": "notes/theme_guide.html#responsive-design",
    "href": "notes/theme_guide.html#responsive-design",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "The theme includes mobile breakpoints at 768px. To customize:\n@media (max-width: 768px) {\n  // Your mobile styles here\n  h1 {\n    font-size: 2rem;  // Smaller on mobile\n  }\n}\n\n// Add additional breakpoints:\n@media (max-width: 1200px) {\n  // Tablet styles\n}\n\n@media (max-width: 480px) {\n  // Small mobile styles\n}"
  },
  {
    "objectID": "notes/theme_guide.html#where-apricots-font-is-used",
    "href": "notes/theme_guide.html#where-apricots-font-is-used",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "Currently applied to: - Main site title “The BioPod” (home page) - Section headings like “Lately…” - Navbar brand (if you add one)\nTo apply elsewhere, use:\n&lt;h1 style=\"font-family: 'Apricots', cursive;\"&gt;Your Title&lt;/h1&gt;\nOr add a class in the theme:\n.apricots-heading {\n  font-family: 'Apricots', cursive;\n}\nThen use: &lt;h1 class=\"apricots-heading\"&gt;Title&lt;/h1&gt;"
  },
  {
    "objectID": "notes/theme_guide.html#troubleshooting",
    "href": "notes/theme_guide.html#troubleshooting",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "Check the font file exists at: fonts/apricots.woff2\nVerify _quarto.yml has fonts/ listed under project: resources:\nThe path in @font-face should be: url('fonts/apricots.woff2')\nClear your browser cache (Ctrl+Shift+R)\nRe-run quarto preview\n\n\n\n\n\nMake sure you edited the $variable in the defaults section\nClear your browser cache (Ctrl+Shift+R)\nRe-run quarto preview\n\n\n\n\n\nCheck if inline styles are overriding theme styles\nUse .very-tight-spacing class for tight spacing\nAdjust $spacer variable for global spacing changes\n\n\n\n\n\nVerify _quarto.yml has: theme: custom-theme.scss\nCheck for SCSS syntax errors\nRun quarto render to see error messages"
  },
  {
    "objectID": "notes/theme_guide.html#pro-tips",
    "href": "notes/theme_guide.html#pro-tips",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "Use Variables: Define colors/spacing as variables at the top, use them throughout\nMobile First: Test on mobile as you build\nSpacing Classes: Use .tight-spacing and .very-tight-spacing liberally\nComments: Add comments in your SCSS to remember what things do\nVersion Control: Commit changes incrementally so you can roll back if needed"
  },
  {
    "objectID": "notes/theme_guide.html#resources",
    "href": "notes/theme_guide.html#resources",
    "title": "Custom Quarto Theme Setup - The BioPod",
    "section": "",
    "text": "Quarto Themes: https://quarto.org/docs/output-formats/html-themes.html\nSass Basics: https://sass-lang.com/guide\nBootstrap Icons: https://icons.getbootstrap.com/\nFont Formats: https://web.dev/learn/design/web-fonts/\n\n\nNeed Help? Check the Quarto docs or edit custom-theme.scss directly. The file has comments explaining each section."
  },
  {
    "objectID": "notes/setup-guide.html",
    "href": "notes/setup-guide.html",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "A comprehensive guide to creating a Quarto blog from scratch, with or without the BioPod custom theme.\n\n\n\n\nQuick Start\nSetting Up Quarto from Scratch\nUsing the BioPod Theme\nCreating Your Own Custom Theme\nKey Components Explained\nTips & Tricks\nCommon Issues & Solutions\n\n\n\n\n\nIf you just want to use the BioPod theme for your own blog:\n\nCopy these files to your Quarto project:\n\nbiopod-theme.scss (the custom theme)\nfonts/ directory (if using custom fonts)\n\nUpdate your _quarto.yml:\nformat:\n  html:\n    theme: biopod-theme.scss\nRender your site:\nquarto render\n\n\n\n\n\n\n\n\nQuarto installed on your system\nA text editor (VS Code, RStudio, etc.)\nBasic understanding of Markdown\n\n\n\n\n# Create project directory\nmkdir my-blog\ncd my-blog\n\n# Initialize a Quarto website\nquarto create project website .\n\n\n\nA minimal Quarto blog needs:\nmy-blog/\n├── _quarto.yml          # Site configuration\n├── index.qmd            # Home page\n├── about.qmd            # About page\n├── posts/               # Blog posts folder\n│   └── post1/\n│       └── index.qmd\n└── images/              # Images folder\n\n\n\nThis is your site’s control center:\nproject:\n  type: website\n  output-dir: docs        # For GitHub Pages\n\nwebsite:\n  title: \"My Blog\"\n  navbar:\n    background: \"#e0cdc5\"\n    left:\n      - text: \"HOME\"\n        href: index.qmd\n      - text: \"BLOG\"\n        href: posts.qmd\n      - text: \"ABOUT\"\n        href: about.qmd\n\nformat:\n  html:\n    theme: default        # Or your custom theme\n    toc: true\n    toc-location: right\n\n\n\nCreate posts/my-first-post/index.qmd:\n---\ntitle: \"My First Post\"\nauthor: \"Your Name\"\ndate: \"2025-01-28\"\ncategories: [news, tutorial]\ndescription: \"A brief description\"\n---\n\n## Introduction\n\nYour content here...\n\n\n\n# Preview locally\nquarto preview\n\n# Build for production\nquarto render\n\n\n\n\n\n\n\nBioPod is a warm, academic-styled theme designed for science and research blogs. It features:\n\nSoft beige/cream color palette (#e0cdc5, #d5c3b3)\nCustom fonts (Crimson Text + Apricots display font)\nClean, readable code blocks\nResponsive design\nCustomizable page backgrounds\n\n\n\n\n\nCopy theme file:\n# Copy biopod-theme.scss to your project root\ncp path/to/biopod-theme.scss ./\nCopy fonts (if using custom fonts):\n# Copy the fonts directory\ncp -r path/to/fonts ./\nUpdate _quarto.yml:\nformat:\n  html:\n    theme: biopod-theme.scss\n\n\n\n\nThe theme is built with SCSS variables that are easy to modify:\n// Brand Colors (lines 16-19 in biopod-theme.scss)\n$bg-primary: #e0cdc5;      // Main background (header/footer)\n$bg-secondary: #d5c3b3;    // Page backgrounds\n$text-heading: #323619;    // Headings color\n$text-body: #000000;       // Body text color\nTo customize: 1. Open biopod-theme.scss 2. Find the /*-- scss:defaults --*/ section at the top 3. Modify the color variables 4. Run quarto render to see changes\n\n\n\n\n\n\n\nQuarto uses SCSS (Sassy CSS) for theming. A theme file has two main sections:\n/*-- scss:defaults --*/\n// Variables and settings\n\n/*-- scss:rules --*/\n// Custom CSS rules\n\n\n\nCreate my-theme.scss in your project root:\n/*-- scss:defaults --*/\n\n// Import fonts\n@import url('https://fonts.googleapis.com/css2?family=Your+Font&display=swap');\n\n// Define colors\n$bg-primary: #yourcolor;\n$text-body: #yourcolor;\n$link-color: #yourcolor;\n\n// Typography\n$font-family-sans-serif: 'Your Font', sans-serif;\n$font-size-base: 1rem;\n\n/*-- scss:rules --*/\n\n// Custom styling\nbody {\n  font-family: $font-family-sans-serif;\n  background-color: $bg-primary;\n}\n\n\n\nColors: - $body-bg - Page background - $body-color - Text color - $link-color - Link color - $navbar-bg - Navigation background\nTypography: - $font-family-sans-serif - Body font - $headings-font-family - Heading font - $font-size-base - Base font size - $h1-font-size through $h6-font-size - Heading sizes\nCode Blocks: - $code-bg - Code background - $code-color - Code text color\n\n\n\nOption 1: Google Fonts\n@import url('https://fonts.googleapis.com/css2?family=Crimson+Text&display=swap');\n\n$font-family-sans-serif: 'Crimson Text', serif;\nOption 2: Local Fonts\n@font-face {\n  font-family: 'MyFont';\n  src: url('fonts/myfont.woff2') format('woff2');\n  font-weight: normal;\n  font-style: normal;\n}\n\n$font-family-sans-serif: 'MyFont', sans-serif;\n\n\n\n\n\n\n\nQuarto offers several page layouts:\n---\ntitle: \"My Page\"\npage-layout: full     # Options: default, full, article\n---\n\ndefault - Standard width with margins\nfull - Full width of page\narticle - Optimized for reading (narrower)\n\n\n\n\nEnable navigation sidebar:\n---\ntitle: \"My Page\"\ntoc: true\ntoc-location: right    # Options: right, left, body\ntoc-depth: 3           # How many heading levels to show\n---\nImportant: TOC won’t generate if headings are inside custom div wrappers (:::).\n\n\n\nCreate posts.qmd for a blog index:\n---\ntitle: \"Blog\"\nlisting:\n  contents: posts          # Folder to scan\n  sort: \"date desc\"\n  type: default           # Options: default, grid, table\n  categories: true\n  fields: [date, title, description]\n  date-format: \"DD MMMM YYYY\"\n---\n\n\n\nAdd custom CSS classes to pages:\n---\ntitle: \"About\"\n---\n\n::: {.custom-class}\nYour content here\n:::\nThen style in your theme:\n.custom-class {\n  background-color: #yourcolor;\n}\n\n\n\n\n\n\n\nUse live preview while developing:\nquarto preview\nChanges to .qmd files reload automatically. For theme changes (.scss), you may need to refresh manually.\n\n\n\nKeep images in logical folders:\nimages/          # Site-wide images (logo, hero)\nposts/\n  post1/\n    images/      # Post-specific images\nReference images relative to the file:\n![Description](images/photo.jpg)\n\n\n\nMake code readable with proper contrast:\npre {\n  background-color: #f5f5f5;\n  border-left: 4px solid $text-heading;\n\n  code {\n    color: #1a1a1a;      // Dark text\n    font-weight: 600;    // Bold for clarity\n  }\n}\n\n\n\nAlways test mobile views. Add media queries:\n@media (max-width: 768px) {\n  .navbar-brand {\n    font-size: 1.5rem !important;\n  }\n}\n\n\n\nUse CSS selectors:\n// Hide TOC on about page only\n.about-page #quarto-margin-sidebar {\n  display: none !important;\n}\n\n\n\nStyle navbar in _quarto.yml:\nnavbar:\n  background: \"#e0cdc5\"\n  foreground: \"#323619\"\n  left:\n    - text: \"HOME\"\n      href: index.qmd\n\n\n\nTarget pages by unique IDs:\n// Resources page has unique section ID\nbody:has(#machine-learning-deep-learning) {\n  background-color: $bg-secondary;\n}\n\n\n\n\n\n\n\nCause: Headings wrapped in div containers\nSolution: Move headings outside custom divs:\n---\ntitle: \"My Page\"\ntoc: true\n---\n\nContent introduction here.\n\n## First Section\n\nMore content...\n\n\n\nCause: Incorrect path or format\nSolution: 1. Check font file exists: ls fonts/yourfont.woff2 2. Verify path in SCSS: url('fonts/yourfont.woff2') 3. Use font-display: swap for better loading\n\n\n\nCause: Browser cache or render cache\nSolution:\n# Clear Quarto cache\nquarto render --no-cache\n\n# Hard refresh browser (Ctrl+Shift+R or Cmd+Shift+R)\n\n\n\nCause: Image field not in listing config OR image path incorrect\nSolution: 1. Add to posts.qmd: yaml    listing:      fields: [image, date, title, description] 2. Add to post frontmatter: yaml    image: images/thumbnail.jpg\n\n\n\nCause: Specificity issues\nSolution: Use !important sparingly:\n.my-class {\n  color: red !important;\n}\nOr increase specificity:\nbody .my-class h2 {\n  color: red;\n}\n\n\n\n\n\n\n\nMethod 1: Custom CSS Classes\nIn your .qmd file:\n::: {.inspiration-page}\nContent here\n:::\nIn theme:\n.inspiration-page {\n  background-color: $bg-primary;\n\n  h2 {\n    font-family: 'Apricots', cursive;\n  }\n}\nMethod 2: Body Has Selector\nTarget pages by unique content:\nbody:has(#unique-section-id) {\n  background-color: $bg-secondary;\n}\n\n\n\nCreate eye-catching home page layouts:\n::: {.hero-container}\n::: {.hero-text}\n# Your Title\nSubtitle text\n:::\n\n::: {.hero-image-container}\n![](images/hero.png)\n:::\n:::\nStyle with flexbox:\n.hero-container {\n  display: flex;\n  gap: 4rem;\n  align-items: center;\n}\n\n.hero-text {\n  flex: 1;\n}\n\n.hero-image-container {\n  flex: 0 0 55%;\n}\n\n\n\nMix fonts for visual hierarchy:\n// Body text\n$font-family-sans-serif: 'Crimson Text', serif;\n\n// Display/decorative\n.navbar-brand,\n.hero-title {\n  font-family: 'Apricots', cursive !important;\n}\n\n\n\n\n\n\n\n\nSet output directory in _quarto.yml:\nproject:\n  output-dir: docs\nRender site:\nquarto render\nPush to GitHub:\ngit add .\ngit commit -m \"Update blog\"\ngit push\nEnable GitHub Pages:\n\nGo to repository Settings\nPages section\nSource: Deploy from branch\nBranch: main, folder: /docs\n\n\n\n\n\n\nCreate netlify.toml:\n[build]\n  command = \"quarto render\"\n  publish = \"_site\"\nConnect repository to Netlify\nDeploy automatically on push\n\n\n\n\n\n\n\n\nPrimary Background:   #e0cdc5 (warm beige)\nSecondary Background: #d5c3b3 (light tan)\nText Heading:         #323619 (dark olive)\nText Body:            #000000 (black)\n\n\n\n\nBody: Crimson Text (serif)\nDisplay: Apricots (cursive/decorative)\nCode: Courier New (monospace)\n\n\n\n\n\nHome: Secondary (#d5c3b3)\nBlog listing: Secondary\nBlog posts: Primary (#e0cdc5)\nAbout: Primary\nResources: Primary\nInspiration: Primary\n\n\n\n\n\nWarm, academic aesthetic - Soft colors reduce eye strain\nHigh contrast code blocks - Bold, dark text on light gray\nSerif body font - Better for long-form reading\nGenerous spacing - Improves readability\nMinimal borders - Clean, modern look\n\n\n\n\n\n\nComplete BioPod blog structure:\nmy-blog/\n├── _quarto.yml              # Site configuration\n├── biopod-theme.scss        # Custom theme\n├── index.qmd                # Home page\n├── about.qmd                # About page\n├── posts.qmd                # Blog listing page\n├── resources.qmd            # Resources page\n├── inspiration.qmd          # Inspiration page\n├── fonts/                   # Custom fonts\n│   └── apricots.woff2\n├── images/                  # Site images\n│   ├── favicon.png\n│   ├── hero-collage.png\n│   └── profile.jpg\n├── posts/                   # Blog posts\n│   └── my-post/\n│       ├── index.qmd\n│       └── images/\n│           └── thumbnail.jpg\n├── notes/                   # Documentation\n│   └── setup-guide.md\n└── docs/                    # Rendered output (gitignore this)\n\n\n\n\n\nQuarto Documentation\nQuarto Websites Guide\nSCSS/Sass Documentation\nBootstrap SCSS Variables (Quarto uses Bootstrap)\nGoogle Fonts\n\n\n\n\n\nIf you make improvements to the BioPod theme:\n\nTest thoroughly across pages\nDocument color/font changes\nEnsure responsive design works\nUpdate this guide if adding major features\n\n\n\n\n\nThis theme is open source. Feel free to use, modify, and share.\nCredit: Created for The BioPod blog by Larysha Rothmann.\n\nLast updated: 2025-01-28"
  },
  {
    "objectID": "notes/setup-guide.html#table-of-contents",
    "href": "notes/setup-guide.html#table-of-contents",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Quick Start\nSetting Up Quarto from Scratch\nUsing the BioPod Theme\nCreating Your Own Custom Theme\nKey Components Explained\nTips & Tricks\nCommon Issues & Solutions"
  },
  {
    "objectID": "notes/setup-guide.html#quick-start",
    "href": "notes/setup-guide.html#quick-start",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "If you just want to use the BioPod theme for your own blog:\n\nCopy these files to your Quarto project:\n\nbiopod-theme.scss (the custom theme)\nfonts/ directory (if using custom fonts)\n\nUpdate your _quarto.yml:\nformat:\n  html:\n    theme: biopod-theme.scss\nRender your site:\nquarto render"
  },
  {
    "objectID": "notes/setup-guide.html#setting-up-quarto-from-scratch",
    "href": "notes/setup-guide.html#setting-up-quarto-from-scratch",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Quarto installed on your system\nA text editor (VS Code, RStudio, etc.)\nBasic understanding of Markdown\n\n\n\n\n# Create project directory\nmkdir my-blog\ncd my-blog\n\n# Initialize a Quarto website\nquarto create project website .\n\n\n\nA minimal Quarto blog needs:\nmy-blog/\n├── _quarto.yml          # Site configuration\n├── index.qmd            # Home page\n├── about.qmd            # About page\n├── posts/               # Blog posts folder\n│   └── post1/\n│       └── index.qmd\n└── images/              # Images folder\n\n\n\nThis is your site’s control center:\nproject:\n  type: website\n  output-dir: docs        # For GitHub Pages\n\nwebsite:\n  title: \"My Blog\"\n  navbar:\n    background: \"#e0cdc5\"\n    left:\n      - text: \"HOME\"\n        href: index.qmd\n      - text: \"BLOG\"\n        href: posts.qmd\n      - text: \"ABOUT\"\n        href: about.qmd\n\nformat:\n  html:\n    theme: default        # Or your custom theme\n    toc: true\n    toc-location: right\n\n\n\nCreate posts/my-first-post/index.qmd:\n---\ntitle: \"My First Post\"\nauthor: \"Your Name\"\ndate: \"2025-01-28\"\ncategories: [news, tutorial]\ndescription: \"A brief description\"\n---\n\n## Introduction\n\nYour content here...\n\n\n\n# Preview locally\nquarto preview\n\n# Build for production\nquarto render"
  },
  {
    "objectID": "notes/setup-guide.html#using-the-biopod-theme",
    "href": "notes/setup-guide.html#using-the-biopod-theme",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "BioPod is a warm, academic-styled theme designed for science and research blogs. It features:\n\nSoft beige/cream color palette (#e0cdc5, #d5c3b3)\nCustom fonts (Crimson Text + Apricots display font)\nClean, readable code blocks\nResponsive design\nCustomizable page backgrounds\n\n\n\n\n\nCopy theme file:\n# Copy biopod-theme.scss to your project root\ncp path/to/biopod-theme.scss ./\nCopy fonts (if using custom fonts):\n# Copy the fonts directory\ncp -r path/to/fonts ./\nUpdate _quarto.yml:\nformat:\n  html:\n    theme: biopod-theme.scss\n\n\n\n\nThe theme is built with SCSS variables that are easy to modify:\n// Brand Colors (lines 16-19 in biopod-theme.scss)\n$bg-primary: #e0cdc5;      // Main background (header/footer)\n$bg-secondary: #d5c3b3;    // Page backgrounds\n$text-heading: #323619;    // Headings color\n$text-body: #000000;       // Body text color\nTo customize: 1. Open biopod-theme.scss 2. Find the /*-- scss:defaults --*/ section at the top 3. Modify the color variables 4. Run quarto render to see changes"
  },
  {
    "objectID": "notes/setup-guide.html#creating-your-own-custom-theme",
    "href": "notes/setup-guide.html#creating-your-own-custom-theme",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Quarto uses SCSS (Sassy CSS) for theming. A theme file has two main sections:\n/*-- scss:defaults --*/\n// Variables and settings\n\n/*-- scss:rules --*/\n// Custom CSS rules\n\n\n\nCreate my-theme.scss in your project root:\n/*-- scss:defaults --*/\n\n// Import fonts\n@import url('https://fonts.googleapis.com/css2?family=Your+Font&display=swap');\n\n// Define colors\n$bg-primary: #yourcolor;\n$text-body: #yourcolor;\n$link-color: #yourcolor;\n\n// Typography\n$font-family-sans-serif: 'Your Font', sans-serif;\n$font-size-base: 1rem;\n\n/*-- scss:rules --*/\n\n// Custom styling\nbody {\n  font-family: $font-family-sans-serif;\n  background-color: $bg-primary;\n}\n\n\n\nColors: - $body-bg - Page background - $body-color - Text color - $link-color - Link color - $navbar-bg - Navigation background\nTypography: - $font-family-sans-serif - Body font - $headings-font-family - Heading font - $font-size-base - Base font size - $h1-font-size through $h6-font-size - Heading sizes\nCode Blocks: - $code-bg - Code background - $code-color - Code text color\n\n\n\nOption 1: Google Fonts\n@import url('https://fonts.googleapis.com/css2?family=Crimson+Text&display=swap');\n\n$font-family-sans-serif: 'Crimson Text', serif;\nOption 2: Local Fonts\n@font-face {\n  font-family: 'MyFont';\n  src: url('fonts/myfont.woff2') format('woff2');\n  font-weight: normal;\n  font-style: normal;\n}\n\n$font-family-sans-serif: 'MyFont', sans-serif;"
  },
  {
    "objectID": "notes/setup-guide.html#key-components-explained",
    "href": "notes/setup-guide.html#key-components-explained",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Quarto offers several page layouts:\n---\ntitle: \"My Page\"\npage-layout: full     # Options: default, full, article\n---\n\ndefault - Standard width with margins\nfull - Full width of page\narticle - Optimized for reading (narrower)\n\n\n\n\nEnable navigation sidebar:\n---\ntitle: \"My Page\"\ntoc: true\ntoc-location: right    # Options: right, left, body\ntoc-depth: 3           # How many heading levels to show\n---\nImportant: TOC won’t generate if headings are inside custom div wrappers (:::).\n\n\n\nCreate posts.qmd for a blog index:\n---\ntitle: \"Blog\"\nlisting:\n  contents: posts          # Folder to scan\n  sort: \"date desc\"\n  type: default           # Options: default, grid, table\n  categories: true\n  fields: [date, title, description]\n  date-format: \"DD MMMM YYYY\"\n---\n\n\n\nAdd custom CSS classes to pages:\n---\ntitle: \"About\"\n---\n\n::: {.custom-class}\nYour content here\n:::\nThen style in your theme:\n.custom-class {\n  background-color: #yourcolor;\n}"
  },
  {
    "objectID": "notes/setup-guide.html#tips-tricks",
    "href": "notes/setup-guide.html#tips-tricks",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Use live preview while developing:\nquarto preview\nChanges to .qmd files reload automatically. For theme changes (.scss), you may need to refresh manually.\n\n\n\nKeep images in logical folders:\nimages/          # Site-wide images (logo, hero)\nposts/\n  post1/\n    images/      # Post-specific images\nReference images relative to the file:\n![Description](images/photo.jpg)\n\n\n\nMake code readable with proper contrast:\npre {\n  background-color: #f5f5f5;\n  border-left: 4px solid $text-heading;\n\n  code {\n    color: #1a1a1a;      // Dark text\n    font-weight: 600;    // Bold for clarity\n  }\n}\n\n\n\nAlways test mobile views. Add media queries:\n@media (max-width: 768px) {\n  .navbar-brand {\n    font-size: 1.5rem !important;\n  }\n}\n\n\n\nUse CSS selectors:\n// Hide TOC on about page only\n.about-page #quarto-margin-sidebar {\n  display: none !important;\n}\n\n\n\nStyle navbar in _quarto.yml:\nnavbar:\n  background: \"#e0cdc5\"\n  foreground: \"#323619\"\n  left:\n    - text: \"HOME\"\n      href: index.qmd\n\n\n\nTarget pages by unique IDs:\n// Resources page has unique section ID\nbody:has(#machine-learning-deep-learning) {\n  background-color: $bg-secondary;\n}"
  },
  {
    "objectID": "notes/setup-guide.html#common-issues-solutions",
    "href": "notes/setup-guide.html#common-issues-solutions",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Cause: Headings wrapped in div containers\nSolution: Move headings outside custom divs:\n---\ntitle: \"My Page\"\ntoc: true\n---\n\nContent introduction here.\n\n## First Section\n\nMore content...\n\n\n\nCause: Incorrect path or format\nSolution: 1. Check font file exists: ls fonts/yourfont.woff2 2. Verify path in SCSS: url('fonts/yourfont.woff2') 3. Use font-display: swap for better loading\n\n\n\nCause: Browser cache or render cache\nSolution:\n# Clear Quarto cache\nquarto render --no-cache\n\n# Hard refresh browser (Ctrl+Shift+R or Cmd+Shift+R)\n\n\n\nCause: Image field not in listing config OR image path incorrect\nSolution: 1. Add to posts.qmd: yaml    listing:      fields: [image, date, title, description] 2. Add to post frontmatter: yaml    image: images/thumbnail.jpg\n\n\n\nCause: Specificity issues\nSolution: Use !important sparingly:\n.my-class {\n  color: red !important;\n}\nOr increase specificity:\nbody .my-class h2 {\n  color: red;\n}"
  },
  {
    "objectID": "notes/setup-guide.html#advanced-customization",
    "href": "notes/setup-guide.html#advanced-customization",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Method 1: Custom CSS Classes\nIn your .qmd file:\n::: {.inspiration-page}\nContent here\n:::\nIn theme:\n.inspiration-page {\n  background-color: $bg-primary;\n\n  h2 {\n    font-family: 'Apricots', cursive;\n  }\n}\nMethod 2: Body Has Selector\nTarget pages by unique content:\nbody:has(#unique-section-id) {\n  background-color: $bg-secondary;\n}\n\n\n\nCreate eye-catching home page layouts:\n::: {.hero-container}\n::: {.hero-text}\n# Your Title\nSubtitle text\n:::\n\n::: {.hero-image-container}\n![](images/hero.png)\n:::\n:::\nStyle with flexbox:\n.hero-container {\n  display: flex;\n  gap: 4rem;\n  align-items: center;\n}\n\n.hero-text {\n  flex: 1;\n}\n\n.hero-image-container {\n  flex: 0 0 55%;\n}\n\n\n\nMix fonts for visual hierarchy:\n// Body text\n$font-family-sans-serif: 'Crimson Text', serif;\n\n// Display/decorative\n.navbar-brand,\n.hero-title {\n  font-family: 'Apricots', cursive !important;\n}"
  },
  {
    "objectID": "notes/setup-guide.html#publishing-your-blog",
    "href": "notes/setup-guide.html#publishing-your-blog",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Set output directory in _quarto.yml:\nproject:\n  output-dir: docs\nRender site:\nquarto render\nPush to GitHub:\ngit add .\ngit commit -m \"Update blog\"\ngit push\nEnable GitHub Pages:\n\nGo to repository Settings\nPages section\nSource: Deploy from branch\nBranch: main, folder: /docs\n\n\n\n\n\n\nCreate netlify.toml:\n[build]\n  command = \"quarto render\"\n  publish = \"_site\"\nConnect repository to Netlify\nDeploy automatically on push"
  },
  {
    "objectID": "notes/setup-guide.html#biopod-theme-reference",
    "href": "notes/setup-guide.html#biopod-theme-reference",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Primary Background:   #e0cdc5 (warm beige)\nSecondary Background: #d5c3b3 (light tan)\nText Heading:         #323619 (dark olive)\nText Body:            #000000 (black)\n\n\n\n\nBody: Crimson Text (serif)\nDisplay: Apricots (cursive/decorative)\nCode: Courier New (monospace)\n\n\n\n\n\nHome: Secondary (#d5c3b3)\nBlog listing: Secondary\nBlog posts: Primary (#e0cdc5)\nAbout: Primary\nResources: Primary\nInspiration: Primary\n\n\n\n\n\nWarm, academic aesthetic - Soft colors reduce eye strain\nHigh contrast code blocks - Bold, dark text on light gray\nSerif body font - Better for long-form reading\nGenerous spacing - Improves readability\nMinimal borders - Clean, modern look"
  },
  {
    "objectID": "notes/setup-guide.html#file-structure-reference",
    "href": "notes/setup-guide.html#file-structure-reference",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Complete BioPod blog structure:\nmy-blog/\n├── _quarto.yml              # Site configuration\n├── biopod-theme.scss        # Custom theme\n├── index.qmd                # Home page\n├── about.qmd                # About page\n├── posts.qmd                # Blog listing page\n├── resources.qmd            # Resources page\n├── inspiration.qmd          # Inspiration page\n├── fonts/                   # Custom fonts\n│   └── apricots.woff2\n├── images/                  # Site images\n│   ├── favicon.png\n│   ├── hero-collage.png\n│   └── profile.jpg\n├── posts/                   # Blog posts\n│   └── my-post/\n│       ├── index.qmd\n│       └── images/\n│           └── thumbnail.jpg\n├── notes/                   # Documentation\n│   └── setup-guide.md\n└── docs/                    # Rendered output (gitignore this)"
  },
  {
    "objectID": "notes/setup-guide.html#resources",
    "href": "notes/setup-guide.html#resources",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "Quarto Documentation\nQuarto Websites Guide\nSCSS/Sass Documentation\nBootstrap SCSS Variables (Quarto uses Bootstrap)\nGoogle Fonts"
  },
  {
    "objectID": "notes/setup-guide.html#contributing-to-biopod-theme",
    "href": "notes/setup-guide.html#contributing-to-biopod-theme",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "If you make improvements to the BioPod theme:\n\nTest thoroughly across pages\nDocument color/font changes\nEnsure responsive design works\nUpdate this guide if adding major features"
  },
  {
    "objectID": "notes/setup-guide.html#license",
    "href": "notes/setup-guide.html#license",
    "title": "Setting Up a Quarto Blog with BioPod Theme",
    "section": "",
    "text": "This theme is open source. Feel free to use, modify, and share.\nCredit: Created for The BioPod blog by Larysha Rothmann.\n\nLast updated: 2025-01-28"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Unsupervised learning: finding patterns without labels\n\n\nAn introduction to clustering methods in biological data analysis\n\n\n\n\n\n29 July 2025\n\n\n\n\n\n\n\nUnderstanding neural networks without the math (yet)\n\n\nA gentle introduction to deep learning and where it fits in the machine learning landscape\n\n\n\n\n\n28 June 2025\n\n\n\n\n\n\n\nVersion control with Git: from zero to hero\n\n\nA practical guide to version control - why you need it, how to use it, and what’s actually happening under the hood\n\n\n\n\n\n04 May 2025\n\n\n\n\n\n\n\nBuilding a Personal Academic Site with Quarto\n\n\nHow I set up this blog using Quarto and a custom theme—and why you might want to do the same\n\n\n\n\n\n20 April 2025\n\n\n\n\n\n\n\nGetting started with the CHPC: a practical guide\n\n\nA bare-bones guide to getting up and running on South Africa’s Centre for High Performance Computing\n\n\n\n\n\n04 March 2025\n\n\n\n\n\n\n\nA deep dive into PCA for biological data\n\n\nA comprehensive walkthrough of Principal Component Analysis from theory to implementation in R\n\n\n\n\n\n22 February 2025\n\n\n\n\n\n\n\nSupervised learning: teaching algorithms to learn from example\n\n\nUnpacking the mathematics behind supervised learning - from linear regression to gradient descent to avoiding the common pitfalls\n\n\n\n\n\n28 January 2025\n\n\n\n\n\n\n\nWSL for Bioinformatics: Setting Up a Functional (and Pretty) Environment\n\n\nA practical guide to setting up Windows Subsystem for Linux for bioinformatics work—from installation to prettification\n\n\n\n\n\n28 January 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "Bioinformatician & writer\n\n\nLarysha Rothmann\n\nI’m Larysha, a bioinformatics PhD student at Stellenbosch University in South Africa. My work involves teaching computers to understand how grapevines handle drought - specifically, which genetic variants matter enough to test in the lab rather than just existing in massive datasets. This blog is where I make sense of what I’m learning because I enjoy making technical work accessible without stripping out what makes it interesting. If you’re curious about computational biology, you might find something useful here.\nBesides my work, I’m interested in too many things for my own good, but the hobbies that I’ve stuck with include sketching, yoga and making sourdough. And if you’d like to have a chin-wag my links are below:\n\n GitHub  LinkedIn  Email"
  },
  {
    "objectID": "inspiration.html",
    "href": "inspiration.html",
    "title": "Inspiration",
    "section": "",
    "text": "“As humans we are inclined to feel that life must have a point. We have plans and aspirations and desires. We want to take constant advantage of all the intoxicating existence we’ve been endowed with. But what’s life to a lichen? Yet its impulse to exist, to be, is every bit as strong as ours—arguably even stronger. If I were told that I had to spend decades being a furry growth on a rock in the woods, I believe I would lose the will to go on. Lichens don’t. Like virtually all living things, they will suffer any hardship, endure any insult, for a moment’s additional existence. Life, in short, just wants to be.”\n― Bill Bryson, A Short History of Nearly Everything\n“Geologists are never at a loss for paperweights.”\n\n― Bill Bryson\n\n“Nature has given us a brain that survives in a changing world by changing itself”\n— Norman Doidge\n“Gradually, the observer realizes that organisms are connected to each other. Not linearly but in a net-like, entangled fabric.”\n— Alexander von Humboldt\n“To use the world well, to be able to stop wasting it and our time in it, we need to re-learn our being in it.”\n— Ursula Le Guin\n“The difference between animals and fungi is simple: Animals put food in their bodies, whereas fungi put their bodies in the food.”\n— Merlin Sheldrake, Entangled Life\n“Science isn’t an exercise in cold-blooded rationality. Scientists are and always have been emotional, creative, intuitive whole human beings asking questions about a world that was never meant to be cataloged and systematized.”\n— Merlin Sheldrake, Entangled Life\n\n“Science is not about building a body of known facts. It is a method for asking awkward questions and subjecting them to reality check, thus avoiding the human tendency to believe whatever makes us feel good.”\n— Terry Pratchett, The Science of The Discworld\n“The problem, of course, with having an open mind is that people will insist on coming along and trying to put things in it.”\n— Terry Pratchett\n“It doesn’t stop being magic just because you know how it works.”\n— Terry Pratchett\n\n“For better or worse, intelligence can come to nothing when the emotions hold sway.”\n― Daniel Goleman, Emotional Intelligence\n“If you want to lift a hundred pounds, you don’t expect to succeed the first time. You start with a lighter weight and work up little by little. You actually fail to lift a hundred pounds, every day, until the day you succeed.”\n― Norman Doidge, The Brain that Changes Itself: Stories of Personal Triumph from the Frontiers of Brain Science\n\n“The soul becomes dyed with the colour of its thoughts.”\n― Marcus Aurelius, Meditations\n“The first rule is to keep an untroubled spirit. The second is to look things in the face and know them for what they are.”\n― Marcus Aurelius\n“Think of yourself as dead. You have lived your life. Now, take what’s left and live it properly.”\n― Marcus Aurelius\n\n“Yet such is oft the course of deeds that move the wheels of the world: small hands do them because they must, while the eyes of the great are elsewhere.”\n― J.R.R. Tolkien, The Lord of the Rings\n“There is a stubbornness about me that never can bear to be frightened at the will of others. My courage always rises at every attempt to intimidate me.”\n― Jane Austen, Pride and Prejudice"
  },
  {
    "objectID": "inspiration.html#snippets-from-people-that-knew-how-to-write",
    "href": "inspiration.html#snippets-from-people-that-knew-how-to-write",
    "title": "Inspiration",
    "section": "",
    "text": "“As humans we are inclined to feel that life must have a point. We have plans and aspirations and desires. We want to take constant advantage of all the intoxicating existence we’ve been endowed with. But what’s life to a lichen? Yet its impulse to exist, to be, is every bit as strong as ours—arguably even stronger. If I were told that I had to spend decades being a furry growth on a rock in the woods, I believe I would lose the will to go on. Lichens don’t. Like virtually all living things, they will suffer any hardship, endure any insult, for a moment’s additional existence. Life, in short, just wants to be.”\n― Bill Bryson, A Short History of Nearly Everything\n“Geologists are never at a loss for paperweights.”\n\n― Bill Bryson\n\n“Nature has given us a brain that survives in a changing world by changing itself”\n— Norman Doidge\n“Gradually, the observer realizes that organisms are connected to each other. Not linearly but in a net-like, entangled fabric.”\n— Alexander von Humboldt\n“To use the world well, to be able to stop wasting it and our time in it, we need to re-learn our being in it.”\n— Ursula Le Guin\n“The difference between animals and fungi is simple: Animals put food in their bodies, whereas fungi put their bodies in the food.”\n— Merlin Sheldrake, Entangled Life\n“Science isn’t an exercise in cold-blooded rationality. Scientists are and always have been emotional, creative, intuitive whole human beings asking questions about a world that was never meant to be cataloged and systematized.”\n— Merlin Sheldrake, Entangled Life\n\n“Science is not about building a body of known facts. It is a method for asking awkward questions and subjecting them to reality check, thus avoiding the human tendency to believe whatever makes us feel good.”\n— Terry Pratchett, The Science of The Discworld\n“The problem, of course, with having an open mind is that people will insist on coming along and trying to put things in it.”\n— Terry Pratchett\n“It doesn’t stop being magic just because you know how it works.”\n— Terry Pratchett\n\n“For better or worse, intelligence can come to nothing when the emotions hold sway.”\n― Daniel Goleman, Emotional Intelligence\n“If you want to lift a hundred pounds, you don’t expect to succeed the first time. You start with a lighter weight and work up little by little. You actually fail to lift a hundred pounds, every day, until the day you succeed.”\n― Norman Doidge, The Brain that Changes Itself: Stories of Personal Triumph from the Frontiers of Brain Science\n\n“The soul becomes dyed with the colour of its thoughts.”\n― Marcus Aurelius, Meditations\n“The first rule is to keep an untroubled spirit. The second is to look things in the face and know them for what they are.”\n― Marcus Aurelius\n“Think of yourself as dead. You have lived your life. Now, take what’s left and live it properly.”\n― Marcus Aurelius\n\n“Yet such is oft the course of deeds that move the wheels of the world: small hands do them because they must, while the eyes of the great are elsewhere.”\n― J.R.R. Tolkien, The Lord of the Rings\n“There is a stubbornness about me that never can bear to be frightened at the will of others. My courage always rises at every attempt to intimidate me.”\n― Jane Austen, Pride and Prejudice"
  },
  {
    "objectID": "notes/biopod-theme-readme.html",
    "href": "notes/biopod-theme-readme.html",
    "title": "BioPod Theme",
    "section": "",
    "text": "A warm, academic-styled theme for Quarto blogs, designed for science and research content.\n\n\n\nCopy biopod-theme.scss to your Quarto project root\nCopy fonts/ directory (if using custom fonts)\nUpdate your _quarto.yml:\nformat:\n  html:\n    theme: biopod-theme.scss\nRun quarto render\n\n\n\n\n$bg-primary: #e0cdc5;      // Warm beige (headers/footers)\n$bg-secondary: #d5c3b3;    // Light tan (page backgrounds)\n$text-heading: #323619;    // Dark olive (headings)\n$text-body: #000000;       // Black (body text)\n\n\n\n\nBody: Crimson Text (Google Fonts)\nDisplay: Apricots (local custom font - optional)\nCode: Courier New\n\n\n\n\n\nClean, readable design optimized for long-form content\nResponsive layout\nBold, high-contrast code blocks\nCustomizable page backgrounds\nBuilt-in support for blog listings\nTable of contents styling\nNavbar and footer theming\n\n\n\n\nOpen biopod-theme.scss and modify the variables in the /*-- scss:defaults --*/ section:\n// Change colors\n$bg-primary: #yourcolor;\n$text-heading: #yourcolor;\n\n// Change fonts\n$font-family-sans-serif: 'Your Font', sans-serif;\n\n\n\nThe theme supports different backgrounds for different page types:\n\nHome page: Uses $bg-secondary\nBlog listing: Uses $bg-secondary\nIndividual blog posts: Uses $bg-primary\nAbout/Resources/Inspiration: Uses $bg-primary\n\nTo change this behavior, modify the body background rules in the /*-- scss:rules --*/ section.\n\n\n\nRequired: - Quarto 1.2+ - Internet connection (for Google Fonts)\nOptional: - Custom fonts in fonts/ directory\n\n\n\n\n\n\nPlace font files in fonts/ directory\nFont is already configured in theme\nRemove these lines if not using custom display font:\n@font-face {\n  font-family: 'Apricots';\n  src: url('fonts/apricots.woff2') format('woff2');\n}\n\n\n\n\nKeep the Google Fonts import and remove the @font-face declaration and any .navbar-brand custom font styling.\n\n\n\n\n\nChrome/Edge (latest)\nFirefox (latest)\nSafari (latest)\nMobile browsers (iOS Safari, Chrome Mobile)\n\n\n\n\nOpen source - free to use and modify.\n\n\n\nCreated for The BioPod blog by Larysha Rothmann.\nFor detailed setup instructions, see notes/setup-guide.md."
  },
  {
    "objectID": "notes/biopod-theme-readme.html#quick-start",
    "href": "notes/biopod-theme-readme.html#quick-start",
    "title": "BioPod Theme",
    "section": "",
    "text": "Copy biopod-theme.scss to your Quarto project root\nCopy fonts/ directory (if using custom fonts)\nUpdate your _quarto.yml:\nformat:\n  html:\n    theme: biopod-theme.scss\nRun quarto render"
  },
  {
    "objectID": "notes/biopod-theme-readme.html#color-palette",
    "href": "notes/biopod-theme-readme.html#color-palette",
    "title": "BioPod Theme",
    "section": "",
    "text": "$bg-primary: #e0cdc5;      // Warm beige (headers/footers)\n$bg-secondary: #d5c3b3;    // Light tan (page backgrounds)\n$text-heading: #323619;    // Dark olive (headings)\n$text-body: #000000;       // Black (body text)"
  },
  {
    "objectID": "notes/biopod-theme-readme.html#typography",
    "href": "notes/biopod-theme-readme.html#typography",
    "title": "BioPod Theme",
    "section": "",
    "text": "Body: Crimson Text (Google Fonts)\nDisplay: Apricots (local custom font - optional)\nCode: Courier New"
  },
  {
    "objectID": "notes/biopod-theme-readme.html#features",
    "href": "notes/biopod-theme-readme.html#features",
    "title": "BioPod Theme",
    "section": "",
    "text": "Clean, readable design optimized for long-form content\nResponsive layout\nBold, high-contrast code blocks\nCustomizable page backgrounds\nBuilt-in support for blog listings\nTable of contents styling\nNavbar and footer theming"
  },
  {
    "objectID": "notes/biopod-theme-readme.html#customization",
    "href": "notes/biopod-theme-readme.html#customization",
    "title": "BioPod Theme",
    "section": "",
    "text": "Open biopod-theme.scss and modify the variables in the /*-- scss:defaults --*/ section:\n// Change colors\n$bg-primary: #yourcolor;\n$text-heading: #yourcolor;\n\n// Change fonts\n$font-family-sans-serif: 'Your Font', sans-serif;"
  },
  {
    "objectID": "notes/biopod-theme-readme.html#page-background-colors",
    "href": "notes/biopod-theme-readme.html#page-background-colors",
    "title": "BioPod Theme",
    "section": "",
    "text": "The theme supports different backgrounds for different page types:\n\nHome page: Uses $bg-secondary\nBlog listing: Uses $bg-secondary\nIndividual blog posts: Uses $bg-primary\nAbout/Resources/Inspiration: Uses $bg-primary\n\nTo change this behavior, modify the body background rules in the /*-- scss:rules --*/ section."
  },
  {
    "objectID": "notes/biopod-theme-readme.html#dependencies",
    "href": "notes/biopod-theme-readme.html#dependencies",
    "title": "BioPod Theme",
    "section": "",
    "text": "Required: - Quarto 1.2+ - Internet connection (for Google Fonts)\nOptional: - Custom fonts in fonts/ directory"
  },
  {
    "objectID": "notes/biopod-theme-readme.html#font-setup",
    "href": "notes/biopod-theme-readme.html#font-setup",
    "title": "BioPod Theme",
    "section": "",
    "text": "Place font files in fonts/ directory\nFont is already configured in theme\nRemove these lines if not using custom display font:\n@font-face {\n  font-family: 'Apricots';\n  src: url('fonts/apricots.woff2') format('woff2');\n}\n\n\n\n\nKeep the Google Fonts import and remove the @font-face declaration and any .navbar-brand custom font styling."
  },
  {
    "objectID": "notes/biopod-theme-readme.html#browser-support",
    "href": "notes/biopod-theme-readme.html#browser-support",
    "title": "BioPod Theme",
    "section": "",
    "text": "Chrome/Edge (latest)\nFirefox (latest)\nSafari (latest)\nMobile browsers (iOS Safari, Chrome Mobile)"
  },
  {
    "objectID": "notes/biopod-theme-readme.html#license",
    "href": "notes/biopod-theme-readme.html#license",
    "title": "BioPod Theme",
    "section": "",
    "text": "Open source - free to use and modify."
  },
  {
    "objectID": "notes/biopod-theme-readme.html#credits",
    "href": "notes/biopod-theme-readme.html#credits",
    "title": "BioPod Theme",
    "section": "",
    "text": "Created for The BioPod blog by Larysha Rothmann.\nFor detailed setup instructions, see notes/setup-guide.md."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "",
    "text": "Academic blogs have become more accessible than ever, and they’re a great way to keep a personal portfolio and record of your work.\nI recently migrated to Quarto for this site, and while it took some time to set up and learn, I’m happy I did. Not just because of aesthetics —though the BioPod theme you’re looking at is entirely custom— but in how the whole thing works. Faster builds, cleaner code, and a system that makes sense when you’re already working in R or Python.\nThis post walks through how I set up The BioPod using Quarto’s website framework and a custom SCSS theme. It’s aimed at anyone who codes enough to use notebooks but isn’t necessarily a web developer, and who wants a professional site without wrestling WordPress into submission."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#why-setup-a-blog",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#why-setup-a-blog",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "",
    "text": "Academic blogs have become more accessible than ever, and they’re a great way to keep a personal portfolio and record of your work.\nI recently migrated to Quarto for this site, and while it took some time to set up and learn, I’m happy I did. Not just because of aesthetics —though the BioPod theme you’re looking at is entirely custom— but in how the whole thing works. Faster builds, cleaner code, and a system that makes sense when you’re already working in R or Python.\nThis post walks through how I set up The BioPod using Quarto’s website framework and a custom SCSS theme. It’s aimed at anyone who codes enough to use notebooks but isn’t necessarily a web developer, and who wants a professional site without wrestling WordPress into submission."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#what-actually-is-quarto",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#what-actually-is-quarto",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "What Actually Is Quarto?",
    "text": "What Actually Is Quarto?\nQuarto is an open-source scientific publishing system. Think of it as Markdown Pro: write content in .qmd files (Quarto Markdown), and it renders to HTML, PDF, Word docs, presentations, or entire websites.\nHere’s why it matters for science communication:\nIt’s built for data science workflows. You can embed R or Python code directly in your documents, execute it during rendering, and include the outputs (plots, tables, analysis results) automatically. Write once, render everywhere.\nIt’s fast and portable. Static site generation means no database, no server-side processing, just HTML/CSS/JS files that load instantly. Host it on GitHub Pages for free.\nIt handles citations natively. BibTeX integration, cross-references, figure numbering—all the academic infrastructure without third-party plugins.\nIt scales from notebooks to books. Same syntax works for Jupyter-style notebooks, scientific papers, presentations, and full websites. Your lab report and your blog post use the same toolchain.\nThe perks aren’t just in ease of rendering, but also the custom aesthetics you want to add to your site. With Quarto, you can write SCSS that compiles directly to CSS. You control exactly what gets rendered. No mystery CSS from seventeen different plugins competing for specificity. No surprise JavaScript breaking your site after a theme update. Just your code, doing what you told it to.\nFinally, because academic work should be reproducible, your entire site is version-controlled plain text files in Quarto. You can see exactly what changed between versions, roll back mistakes, and rebuild the site identically on any machine with Quarto installed."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#the-technical-architecture",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#the-technical-architecture",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "The Technical Architecture",
    "text": "The Technical Architecture\nHere’s what happens when you build a Quarto site:\nYour content (.qmd files)\n         ↓\nQuarto processes Markdown + code chunks\n         ↓\nExecutes R/Python/Julia code (optional)\n         ↓\nApplies theme (SCSS → CSS)\n         ↓\nRenders to HTML\n         ↓\nStatic site in _site/ or docs/\nEverything from your .qmd source to the final HTML happens in a single render step. The output is pure static files—HTML, CSS, JavaScript, images—that you can host anywhere.\n\nHow SCSS Extends CSS\nStandard CSS is great but repetitive. Want to use the same colour scheme across your site? You’re copying hex codes everywhere. Need to adjust all your heading sizes proportionally? Find-and-replace time.\nSCSS (Sassy CSS) adds programming features to CSS:\nVariables:\n// Define once\n$bg-primary: #e0cdc5;\n$text-heading: #323619;\n\n// Use everywhere\nbody {\n  background-color: $bg-primary;\n}\nh1 {\n  color: $text-heading;\n}\nNesting:\nInstead of writing separate CSS rules for related elements, SCSS lets you nest them hierarchically. This mirrors your HTML structure and makes relationships between elements clearer:\n.navbar {\n  background: $bg-primary;\n  \n  .nav-link {\n    color: $text-heading;\n    \n    &:hover {\n      color: darken($text-heading, 10%);\n    }\n  }\n}\nFunctions and mixins:\nMixins are reusable chunks of CSS that you can include wherever you need them, with optional parameters. They’re perfect for repeated patterns:\n@mixin rounded($radius) {\n  border-radius: $radius;\n  -webkit-border-radius: $radius;\n  -moz-border-radius: $radius;\n}\n\n.box {\n  @include rounded(10px);\n}\nWhen you run quarto render, it compiles your SCSS to standard CSS automatically. You write the human-readable SCSS with variables and logic, browsers get optimised CSS.\n\n\nQuarto’s Theme System\nQuarto uses Bootstrap 5 as its base framework, which means you get a solid foundation of responsive design, typography, and components out of the box. Your custom theme extends Bootstrap by overriding its default variables and adding your own rules.\nA Quarto SCSS theme has two sections:\n/*-- scss:defaults --*/\n// Variable overrides go here\n// These change Bootstrap's default values\n$body-bg: #e0cdc5;\n$body-color: #000000;\n$font-family-sans-serif: 'Crimson Text', serif;\n\n/*-- scss:rules --*/\n// Custom CSS rules go here\n// These add new styles or override specific elements\n.hero-container {\n  display: flex;\n  gap: 4rem;\n}\nThe defaults section runs first, setting up your design variables. Then Bootstrap compiles with your variables. Finally, the rules section applies your custom styles on top.\nThis means you’re not fighting Bootstrap—you’re configuring it, then adding your own layer where needed."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#the-biopod-theme",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#the-biopod-theme",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "The BioPod Theme",
    "text": "The BioPod Theme\nThe theme you’re looking at was built around a few specific goals:\n\nWarm, academic aesthetic — softer colours than the usual stark-white-background blog\nReadable code blocks — high contrast, bold text, clear syntax highlighting\nCustom typography — Crimson Text for body, Apricots for display elements\nFlexible page layouts — different background colours for different page types\n\nHere’s the colour palette:\n$bg-primary: #e0cdc5;      // Warm beige (headers/footers)\n$bg-secondary: #d5c3b3;    // Light tan (page backgrounds)\n$text-heading: #323619;    // Dark olive (headings)\n$text-body: #000000;       // Black (body text)\nIf you want to use the BioPod theme as a starting point, it’s available on GitHub. Please credit the original if you use it, and more importantly, make it your own—tweak the colours, swap the fonts, adjust the spacing. Academic sites should reflect the person behind them."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#setting-up-your-own-quarto-site",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#setting-up-your-own-quarto-site",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "Setting Up Your Own Quarto Site",
    "text": "Setting Up Your Own Quarto Site\n\nPrerequisites\n\nQuarto installed (works on Windows, Mac, Linux)\nA text editor (VS Code, RStudio, whatever you prefer)\nGit (for version control and GitHub Pages deployment)\nA GitHub account\nBasic familiarity with Markdown\n\n\n\nProject Structure\nmy-blog/\n├── _quarto.yml           # Site configuration\n├── custom-theme.scss     # Your theme (optional)\n├── index.qmd             # Home page\n├── about.qmd             # About page\n├── posts.qmd             # Blog listing\n├── posts/                # Blog posts directory\n│   └── post-name/\n│       └── index.qmd\n├── images/               # Site-wide images\n└── .github/              # GitHub Actions workflows\n    └── workflows/\n        └── publish.yml\n\n\nStep 1: Initialise the Project\n# Create project directory\nmkdir my-blog\ncd my-blog\n\n# Initialise Quarto website\nquarto create project website .\nThis generates a basic site structure with _quarto.yml, index.qmd, and about.qmd.\n\n\nStep 2: Configure _quarto.yml\nThis file controls your entire site:\nproject:\n  type: website\n\nwebsite:\n  title: \"Your Site Name\"\n  navbar:\n    background: \"#e0cdc5\"\n    left:\n      - text: \"HOME\"\n        href: index.qmd\n      - text: \"BLOG\"\n        href: posts.qmd\n      - text: \"ABOUT\"\n        href: about.qmd\n\nformat:\n  html:\n    theme: default        # Or your custom theme\n    toc: true\n    toc-location: right\nNote: We’re not setting output-dir: docs here because we’ll be using GitHub Actions to build the site automatically.\n\n\nStep 3: Create Your First Post\nCreate posts/my-first-post/index.qmd:\n---\ntitle: \"My First Post\"\nauthor: \"Your Name\"\ndate: \"2025-01-28\"\ncategories: [news, tutorial]\ndescription: \"A brief description for the blog listing\"\n---\n\n## Introduction\n\nYour content here. Write in Markdown, embed code, include images.\n\n\nStep 4: Set Up Blog Listing\nCreate posts.qmd to display all posts:\n---\ntitle: \"Blog\"\nlisting:\n  contents: posts\n  sort: \"date desc\"\n  type: default\n  categories: true\n  fields: [date, title, description]\n  date-format: \"DD MMMM YYYY\"\n---\n\n\nStep 5: Preview Locally\nquarto preview\nThis opens your site in a browser and live-reloads as you make changes."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#using-the-biopod-theme",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#using-the-biopod-theme",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "Using the BioPod Theme",
    "text": "Using the BioPod Theme\nIf you want to use the exact theme from this site, you’re welcome to. The theme is available on GitHub for anyone to use and adapt.\nA small request though: if you do use BioPod as your starting point, please credit the original by linking back to this site or the GitHub repo. And more importantly, make it your own. Tweak the colours, swap the fonts, adjust the spacing —academic sites should reflect the person behind them, not just a template. And I’d genuinely love to see what you do differently with it.\nNow, here’s how to get started:\n\nStep 1: Get the Theme Files\nDownload or copy: - biopod-theme.scss (the theme file) - fonts/apricots.woff2 (optional custom font)\nPlace them in your project root.\n\n\nStep 2: Update _quarto.yml\nproject:\n  type: website\n  output-dir: docs\n  resources:\n    - fonts/              # Include fonts directory in output\n\nformat:\n  html:\n    theme: biopod-theme.scss\n    toc: true\n\n\nStep 3: Render\nquarto render\nThat’s it. Your site now uses the BioPod theme."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#customising-the-theme",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#customising-the-theme",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "Customising the Theme",
    "text": "Customising the Theme\nWant to tweak colours or fonts? Open biopod-theme.scss and edit the variables in the /*-- scss:defaults --*/ section:\n/*-- scss:defaults --*/\n\n// Brand colours\n$bg-primary: #e0cdc5;      // Change this to your preferred colour\n$bg-secondary: #d5c3b3;    // And this\n$text-heading: #323619;    // And this\n$text-body: #000000;       // You get the idea\n\n// Typography\n$font-family-sans-serif: 'Crimson Text', serif;  // Or your font\n$h1-font-size: 2.5rem;     // Adjust heading sizes\n$h2-font-size: 2rem;\nThen re-render:\nquarto render\nThe changes apply immediately. No cache clearing, no fighting with specificity—just change the variable, rebuild, done.\n\nAdding Your Own Fonts\nGoogle Fonts (easiest):\n@import url('https://fonts.googleapis.com/css2?family=Your+Font&display=swap');\n\n$font-family-sans-serif: 'Your Font', sans-serif;\nLocal fonts (more control):\n\nPlace font file (.woff2 preferred) in fonts/your-font.woff2\nAdd to theme:\n\n@font-face {\n  font-family: 'Your Font';\n  src: url('fonts/your-font.woff2') format('woff2');\n  font-weight: normal;\n  font-style: normal;\n  font-display: swap;\n}\n\n$font-family-sans-serif: 'Your Font', sans-serif;\n\nMake sure fonts/ is included in _quarto.yml resources (see Step 2 above)\n\n\n\nPage-Specific Styling\nDifferent pages can have different backgrounds or layouts. The BioPod theme uses CSS selectors to target specific pages:\n// Home page and blog listing get secondary background\nbody:has(.quarto-title-block) {\n  background-color: $bg-secondary;\n}\n\n// Individual blog posts get primary background\nbody:has(article) {\n  background-color: $bg-primary;\n}\nYou can extend this pattern to style any page uniquely based on its content or metadata."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#deploying-to-github-pages",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#deploying-to-github-pages",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "Deploying to GitHub Pages",
    "text": "Deploying to GitHub Pages\nThere are two ways to deploy a Quarto site to GitHub Pages:\n\nMethod 1: Manual Rendering (Simpler, But More Steps)\n\nSet output-dir: docs in _quarto.yml\nRun quarto render locally\nCommit the docs/ folder to git\nConfigure GitHub Pages to serve from the docs/ folder\n\nYour workflow becomes: Edit → Render → Add → Commit → Push\nThis is straightforward but means you need to remember to render before every push, and you’re committing generated HTML files to version control.\n\n\nMethod 2: GitHub Actions (Automated)\nGitHub Actions builds your site automatically on every push. You only commit your source .qmd files, and GitHub handles the rendering.\nYour workflow becomes: Edit → Add → Commit → Push\nThis is what I use, and it’s what I’ll walk through here. If you prefer the manual method, the Quarto documentation covers it in detail."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#what-you-get",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#what-you-get",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "What You Get",
    "text": "What You Get\nAfter following this setup, you’ll have:\n\nA fast, clean static site hosted for free on GitHub Pages\nAutomated deployment that rebuilds on every push\nVersion-controlled content as plain text files\nFull control over design and functionality via SCSS (if you want it)\nNative support for code execution, citations, and cross-references\nA publishing workflow that integrates with your research tools\n\nIt’s not the easiest option - WordPress is still easier if you’ve never touched code. But if you’re already working in R or Python, and you want a professional site that you actually understand and control, Quarto hits a sweet spot.\nThe initial setup takes an afternoon. After that, writing and publishing a new post takes minutes."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#further-resources",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#further-resources",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "Further Resources",
    "text": "Further Resources\n\nQuarto documentation\nQuarto websites guide\nGitHub Pages deployment options\nSCSS/theming deep dive\nBioPod theme source"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "The BioPod\n\n\nall things genomics, bioinformatics & sustainability\n\nComputational work is often a solo activity-just you, your code, and a gene network. But the big challenges aren’t tackled alone: crops failing, climates shifting, farmers adapting. These need collective solutions.\nThis blog is my way of connecting the dots and engaging beyond the screen. I write about the technical stuff I’m learning and the broader issues it touches (and some other things inbetween). Light on jargon, heavy on curiosity.\n\n\n\n\n\n\n\nLately…\n\n\n\n\n\n\n\n\nUnsupervised learning: finding patterns without labels\n\n\nAn introduction to clustering methods in biological data analysis\n\n\n\n29 Jul 2025\n\n\n\n\n\n\n\n\n\n\nUnderstanding neural networks without the math (yet)\n\n\nA gentle introduction to deep learning and where it fits in the machine learning landscape\n\n\n\n28 Jun 2025\n\n\n\n\n\n\n\n\n\n\nVersion control with Git: from zero to hero\n\n\nA practical guide to version control - why you need it, how to use it, and what’s actually happening under the hood\n\n\n\n04 May 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#the-technical-architecture-brief-version",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#the-technical-architecture-brief-version",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "The Technical Architecture (Brief Version)",
    "text": "The Technical Architecture (Brief Version)\nHere’s what happens when you build a Quarto site:\nYour content (.qmd files)\n         ↓\nQuarto processes Markdown + code chunks\n         ↓\nExecutes R/Python/Julia code (optional)\n         ↓\nApplies theme (SCSS → CSS)\n         ↓\nRenders to HTML\n         ↓\nStatic site in _site/ or docs/\nEverything from your .qmd source to the final HTML happens in a single render step. The output is pure static files—HTML, CSS, JavaScript, images—that you can host anywhere.\nQuarto uses Bootstrap 5 as its base framework, and you extend it with SCSS variables and custom rules. If you want to dig into the details of how SCSS works, custom fonts, and advanced theming, the Quarto theming documentation covers it comprehensively."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#setting-up-github-actions-deployment",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#setting-up-github-actions-deployment",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "Setting Up GitHub Actions Deployment",
    "text": "Setting Up GitHub Actions Deployment\n\nStep 1: Create the Workflow File\nIn your project directory, create .github/workflows/publish.yml:\nmkdir -p .github/workflows\nnano .github/workflows/publish.yml\nPaste this workflow configuration:\nname: Publish Quarto Site\n\non:\n  push:\n    branches: main\n\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n      \n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n      \n      - name: Render site\n        run: quarto render\n      \n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: ./_site\n  \n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    \n    permissions:\n      pages: write\n      id-token: write\n    \n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    \n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\nWhat this does:\n\nTriggers on every push to the main branch\nSpins up an Ubuntu virtual machine\nRuns quarto render to build your site\nUploads the built site\nDeploys to GitHub Pages\n\n\n\nStep 2: Create Your GitHub Repository\n\nGo to GitHub and create a new repository\nName it something like your-blog\nMake it Public (required for free GitHub Pages)\nDon’t initialize with README, .gitignore, or license\n\n\n\nStep 3: Push Your Code to GitHub\nIn your project directory:\n# Initialize git (if not already done)\ngit init\n\n# Add all files\ngit add .\n\n# Commit\ngit commit -m \"Initial commit: Quarto blog with GitHub Actions deployment\"\n\n# Link to your GitHub repository\ngit remote add origin https://github.com/yourusername/your-blog.git\n\n# Push to GitHub\ngit branch -M main\ngit push -u origin main\nYou’ll need a Personal Access Token (classic) instead of your password for HTTPS authentication.\n\n\nStep 4: Configure GitHub Pages\n\nGo to your repository on GitHub\nSettings → Pages\nUnder “Build and deployment”:\n\nSource: GitHub Actions (not “Deploy from a branch”)\n\nThat’s it - no need to select a branch or folder\n\n\n\nStep 5: Watch It Build\n\nGo to the Actions tab in your repository\nYou’ll see a workflow run called “Publish Quarto Site”\nAfter 2-3 minutes, your site will be live at https://yourusername.github.io/your-blog/"
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#your-publishing-workflow",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#your-publishing-workflow",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "Your Publishing Workflow",
    "text": "Your Publishing Workflow\nFrom now on, whenever you want to update your blog:\n# 1. Edit your .qmd files\n# 2. Preview locally to check everything looks right\nquarto preview\n\n# 3. Commit and push\ngit add .\ngit commit -m \"Add new blog post about gene regulatory networks\"\ngit push\nThat’s it. GitHub Actions handles the rendering automatically. Within a few minutes, your changes are live.\nNo need to run quarto render yourself, no need to commit the generated HTML files. Just edit your source files and push."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#why-i-chose-github-actions",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#why-i-chose-github-actions",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "Why I Chose GitHub Actions",
    "text": "Why I Chose GitHub Actions\nThe manual approach (rendering locally and committing the docs/ folder) works fine, but I prefer GitHub Actions for a few reasons:\n\nCleaner git history. You only commit source files, not generated HTML. Your diffs show actual content changes, not thousands of lines of auto-generated code.\nConsistency. Every build happens in the same environment. No “works on my machine” issues because you forgot to update a dependency.\nLess to remember. I can edit a post, commit, and push from my phone if needed. GitHub handles the build.\nBackup safety. If my laptop dies, I can clone the repo on any machine and keep working. The entire site rebuilds from source.\n\nThe trade-off is slightly more complex initial setup (the workflow YAML file), but once it’s configured, it just works. I haven’t touched that file since I set it up."
  },
  {
    "objectID": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#troubleshooting",
    "href": "posts/quarto_blog_setup/quarto-biopod-setup-post.html#troubleshooting",
    "title": "Building a Personal Academic Site with Quarto",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nWorkflow fails with authentication error: - Check that your repository is Public - Verify Settings → Actions → General → Workflow permissions is set to “Read and write permissions”\nSite doesn’t update after push: - Check the Actions tab for error messages - Make sure GitHub Pages source is set to “GitHub Actions” (not “Deploy from a branch”) - Wait 2-3 minutes and hard refresh your browser (Ctrl+Shift+R)\nYAML syntax error: - Indentation must be exact (2 spaces, not tabs) - Copy the workflow file exactly as shown above"
  },
  {
    "objectID": "posts/intro_to_dl/index.html",
    "href": "posts/intro_to_dl/index.html",
    "title": "Understanding neural networks without the math (yet)",
    "section": "",
    "text": "Modern biology generates data at a scale that would have seemed absurd a decade ago. Next- and third-generation sequencing, genome-wide expression studies, multi-omics integration - the bottleneck isn’t data collection anymore, it’s making sense of it all.\nThe challenge is that biological relationships are complex. Gene A affects gene B, which affects gene C, which loops back to influence gene A under certain conditions. Expression patterns shift depending on environment. Regulatory elements interact in ways that simple linear models can’t capture. You need algorithms that can handle complexity, non-linearity, and high dimensionality without requiring you to manually consider every possible interaction.\nThis post isn’t about getting into the technical details of deep learning - that comes later. Instead, it’s about fitting deep learning into the broader context and understanding when it might be useful."
  },
  {
    "objectID": "posts/intro_to_dl/index.html#a-brief-outline-of-deep-learning-in-genomics",
    "href": "posts/intro_to_dl/index.html#a-brief-outline-of-deep-learning-in-genomics",
    "title": "Understanding neural networks without the math (yet)",
    "section": "",
    "text": "Modern biology generates data at a scale that would have seemed absurd a decade ago. Next- and third-generation sequencing, genome-wide expression studies, multi-omics integration - the bottleneck isn’t data collection anymore, it’s making sense of it all.\nThe challenge is that biological relationships are complex. Gene A affects gene B, which affects gene C, which loops back to influence gene A under certain conditions. Expression patterns shift depending on environment. Regulatory elements interact in ways that simple linear models can’t capture. You need algorithms that can handle complexity, non-linearity, and high dimensionality without requiring you to manually consider every possible interaction.\nThis post isn’t about getting into the technical details of deep learning - that comes later. Instead, it’s about fitting deep learning into the broader context and understanding when it might be useful."
  },
  {
    "objectID": "posts/intro_to_dl/index.html#where-does-deep-learning-fit",
    "href": "posts/intro_to_dl/index.html#where-does-deep-learning-fit",
    "title": "Understanding neural networks without the math (yet)",
    "section": "Where Does Deep Learning Fit?",
    "text": "Where Does Deep Learning Fit?\nBefore diving into neural networks, it helps to see where deep learning sits in the broader machine learning ecosystem. It’s part of a continuum or “umbrella” of methods that biologists already use, not something completely separate (and which I have whimsically illustrated below).\n\n\n\nThe machine learning landscape\n\n\nLet’s do a quick tour:\nSupervised learning includes familiar territory - linear regression for modelling growth curves, logistic regression for disease prediction, decision trees for variant classification. These models learn from labelled data: you provide input-output pairs, and they learn the mapping.\nUnsupervised learning is for when you don’t have labels. This is where clustering comes in, such as identifying cell types from expression profiles, and dimensionality reduction, like visualising high-dimensional datasets with PCA or t-SNE. You’re looking for structure in the data itself.\nBayesian methods give you a probabilistic framework for incorporating prior knowledge. GWAS analyses often use Bayesian approaches when you want to integrate what’s already known about genetic associations with new evidence.\nEnsemble learning - random forests, boosting methods like XGBoost - combines multiple simpler models to get better predictions than any single model could achieve. These are workhorses in genomics, particularly for expression prediction and disease risk modelling.\nAnd then there’s deep learning - a subfield that excels at large, complex, high-dimensional data. It includes various architectures, but the ones most relevant for sequence data are convolutional neural networks (CNNs) and recurrent neural networks (RNNs) and, more recently, transformers. You can also get creative and combine architectures into hybrid approaches, depending on the scale of the problem.\nThe point: deep learning isn’t replacing these other methods. It’s another tool, suited to specific kinds of problems where traditional approaches hit their limits."
  },
  {
    "objectID": "posts/intro_to_dl/index.html#why-deep-learning-for-biology",
    "href": "posts/intro_to_dl/index.html#why-deep-learning-for-biology",
    "title": "Understanding neural networks without the math (yet)",
    "section": "Why Deep Learning for Biology?",
    "text": "Why Deep Learning for Biology?\nLet’s use a concrete example: finding regulatory elements in DNA\nTraditionally, researchers have used motif scanning tools to predict where transcription factors bind based on experimental data. These methods work by searching DNA sequences for known patterns - short stretches of nucleotides that match previously experimentally identified binding sites in a closely related organism. While useful for generating large databases of motifs, they come with significant limitations: they can’t discover new regulatory sequences, and by design, they’re reductionist, treating motifs in isolation without much certainty about whether a motif is even functional in context.\nThis is where deep learning becomes powerful. Neural networks can learn local features like short motifs, but they can also capture higher-order patterns - motif spacing, combinations, orientation, and context. Crucially, they don’t require prior knowledge. They learn regulatory features directly from sequence data, discovering patterns that might not match any known motif but still drive gene expression.\nIt’s also become feasible. Hardware advances - particularly GPUs enabling parallel computing - mean you can train models with millions of parameters in hours instead of weeks. Software frameworks like Keras, PyTorch, and TensorFlow have made building and implementing these models much more accessible. You don’t need to code the algorithms from scratch anymore."
  },
  {
    "objectID": "posts/intro_to_dl/index.html#the-building-block-whats-a-perceptron",
    "href": "posts/intro_to_dl/index.html#the-building-block-whats-a-perceptron",
    "title": "Understanding neural networks without the math (yet)",
    "section": "The Building Block: What’s a Perceptron?",
    "text": "The Building Block: What’s a Perceptron?\nBefore we can understand neural networks, we need to understand their basic unit: the perceptron.\nThe concept of an artificial neuron traces back to the 1940s, when neurophysiologist Warren McCulloch and logician Walter Pitts proposed a simplified mathematical model loosely inspired by how neurons fire in the brain. Rosenblatt’s perceptron in 1958 refined this into something closer to what we use today.\nHere’s the idea: a perceptron is a computational unit that interprets signals from multiple inputs. Each input has a weight - a learned parameter that determines how much that input matters. The perceptron calculates a weighted sum of its inputs, adds a bias term, and then applies a threshold test. If the result exceeds the threshold, the perceptron “fires” and passes a signal forward. If not, it stays silent. The perceptron learns, through training, which weights to assign to each input.\nIf you want the full history and a clear walkthrough of how perceptrons actually work under the hood, this YouTube video is excellent. It covers the evolution of neural networks from their origins to modern implementations in a way that makes the concept click."
  },
  {
    "objectID": "posts/intro_to_dl/index.html#what-actually-is-a-neural-network",
    "href": "posts/intro_to_dl/index.html#what-actually-is-a-neural-network",
    "title": "Understanding neural networks without the math (yet)",
    "section": "What Actually Is a Neural Network?",
    "text": "What Actually Is a Neural Network?\nNow that we understand perceptrons, a neural network is simply a collection of these computational units arranged into layers. Instead of a single perceptron making a decision, you have multiple perceptrons working together, each learning to recognize different patterns in the data.\nLet’s look at the simplest version: a feed-forward neural network.\n\n\n\nAnatomy of a feed-forward neural network\n\n\nIn the diagram above:\nLayer 0 (input layer): Your feature vector - for example, a DNA sequence that has been numerically encoded in some way.\nLayer 1 (hidden layer): Where the processing happens. Each neuron receives signals from all input neurons, weights them according to “learned” parameters, and “decides” whether to pass the signal forward based on its threshold.\nLayer 2 (output layer): The prediction - perhaps a classification (predicted “high” or “low” expression of a gene, based on the DNA sequence and the “learned” target patterns) or a continuous value (predicted relative expression levels on a scale).\nDuring training, the network compares its predictions to the actual outcomes and adjusts the weights to minimize it’s mistakes or “error”. This happens iteratively, over thousands or millions of training examples, until the model converges on a set of parameters that work.\nThe key insight is that each layer builds on the previous one. The first layer might learn simple patterns (this sequence appears here), the next layer combines those into more complex patterns (these sequences appear together), and so on. The network constructs a hierarchy of features, from simple to complex, automatically."
  },
  {
    "objectID": "posts/intro_to_dl/index.html#the-deep-in-deep-learning",
    "href": "posts/intro_to_dl/index.html#the-deep-in-deep-learning",
    "title": "Understanding neural networks without the math (yet)",
    "section": "The “Deep” in Deep Learning",
    "text": "The “Deep” in Deep Learning\nSo why “deep” learning?\nThe term refers to networks with many layers. More layers means more capacity to learn complex, hierarchical relationships. A shallow network might learn simple patterns (this sequence motif is linked to higher expression), while a deep network can learn combinations of patterns (these three motifs co-occur in this configuration under these conditions).\nBut depth comes at a cost: more layers means more parameters to train. For genomic data, this can mean millions of trainable parameters, which makes these models both data-hungry and computationally intensive."
  },
  {
    "objectID": "posts/intro_to_dl/index.html#whats-next",
    "href": "posts/intro_to_dl/index.html#whats-next",
    "title": "Understanding neural networks without the math (yet)",
    "section": "What’s Next",
    "text": "What’s Next\nThis was the high-level view: what deep learning is, where it fits, and when it’s useful. Future posts will get more technical - how training actually works, strategies for avoiding overfitting, and practical implementation."
  },
  {
    "objectID": "posts/intro_to_dl/index.html#further-reading",
    "href": "posts/intro_to_dl/index.html#further-reading",
    "title": "Understanding neural networks without the math (yet)",
    "section": "Further Reading",
    "text": "Further Reading\n\n3Blue1Brown’s neural network series - exceptional visualisations\nEraslan et al. (2019). “Deep learning: new computational modelling techniques for genomics.” Nature Reviews Genetics"
  },
  {
    "objectID": "posts/unsupervied_learning_intro/index.html",
    "href": "posts/unsupervied_learning_intro/index.html",
    "title": "Unsupervised learning: finding patterns without labels",
    "section": "",
    "text": "In this post’s plot, we explored the context “umbrella” of the machine learning landscape. Now let’s explore another major branch: unsupervised learning.\nUnlike supervised learning - where you provide labelled input-output pairs for training - unsupervised learning identifies patterns in data without predefined categories. You’re not telling the algorithm what to look for; you’re asking it to find structure on its own. This makes it particularly useful during exploratory data analysis, when you don’t yet know what groups or patterns exist in your data.\nThe two primary approaches are clustering, which groups similar data points together, and dimensionality reduction, which simplifies high-dimensional data while preserving meaningful patterns. This post focuses on clustering. Dimensionality reduction with PCA has been covered in a separate post, here."
  },
  {
    "objectID": "posts/unsupervied_learning_intro/index.html#finding-structure-in-unlabelled-data",
    "href": "posts/unsupervied_learning_intro/index.html#finding-structure-in-unlabelled-data",
    "title": "Unsupervised learning: finding patterns without labels",
    "section": "",
    "text": "In this post’s plot, we explored the context “umbrella” of the machine learning landscape. Now let’s explore another major branch: unsupervised learning.\nUnlike supervised learning - where you provide labelled input-output pairs for training - unsupervised learning identifies patterns in data without predefined categories. You’re not telling the algorithm what to look for; you’re asking it to find structure on its own. This makes it particularly useful during exploratory data analysis, when you don’t yet know what groups or patterns exist in your data.\nThe two primary approaches are clustering, which groups similar data points together, and dimensionality reduction, which simplifies high-dimensional data while preserving meaningful patterns. This post focuses on clustering. Dimensionality reduction with PCA has been covered in a separate post, here."
  },
  {
    "objectID": "posts/unsupervied_learning_intro/index.html#clustering-grouping-the-similar",
    "href": "posts/unsupervied_learning_intro/index.html#clustering-grouping-the-similar",
    "title": "Unsupervised learning: finding patterns without labels",
    "section": "Clustering: Grouping the Similar",
    "text": "Clustering: Grouping the Similar\nClustering gives you a sense of the structure and distribution of a dataset. It identifies homogeneous groups based on distances between data points - samples that are “close” by some metric get grouped together. This is one of the most useful techniques during exploratory data analysis in genomics.\nLet’s look at two foundational methods."
  },
  {
    "objectID": "posts/unsupervied_learning_intro/index.html#k-means-clustering",
    "href": "posts/unsupervied_learning_intro/index.html#k-means-clustering",
    "title": "Unsupervised learning: finding patterns without labels",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nK-means is conceptually simple: you specify the number of clusters (\\(K\\)) you want, and the algorithm groups your data accordingly.\nHere’s how it works. The algorithm calculates the Euclidean distance \\(d\\) between data points. For two points \\(p\\) and \\(q\\) with coordinates \\((x_1, y_1)\\) and \\((x_2, y_2)\\):\n\\[d(p, q) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\\]\nSamples with minimal distance between them are assigned to the same cluster. Each cluster has a “centroid” - the centre point of all samples in that cluster. As new points are added, the centroid is recalculated iteratively by computing the mean position of all points in the cluster.\n\nChoosing the Right K\nHow do you know how many clusters to use? One common approach is to calculate the sum of squared errors (SSE) for each cluster:\n\\[{SSE}_k = \\sum_{i=1}^{n_k} (x_i - C_k)^2\\]\nwhere \\(n_k\\) is the number of points in the cluster, \\(C_k\\) is the cluster centroid, and \\((x_i - C_k)^2\\) is the squared Euclidean distance between each point and the centroid.\nYou sum the individual cluster SSE values to get a total SSE (\\(SSE_T\\)), then repeat this for different values of \\(K\\). Plotting \\(SSE_T\\) against \\(K\\) gives you an “elbow plot” - the \\(SSE_T\\) typically decreases as \\(K\\) increases, but at some point the rate of decrease slows down. That inflection point, the “elbow”, suggests the optimal number of clusters.\n\n\n\nElbow plot showing total SSE for varying values of K. The optimal number of clusters is typically where the decrease plateaus - in this case, K = 3 or 4 would be appropriate.\n\n\nThe method is somewhat subjective and often requires domain expertise to interpret correctly. Is the elbow at \\(K\\) = 3 or 4? It depends on your biological question.\n\n\nExample: Gene Expression Under Stress\nHere’s a simple illustration. Suppose you’ve measured expression levels for many genes under drought stress and normal conditions in wheat. Each data point represents a gene, and the axes represent expression levels under the two conditions.\n\n\n\nScatter plots clustered with K=4 (left) and K=3 (right). Each point represents a gene; axes show expression under drought (y-axis) and normal conditions (x-axis). Different values of K may be appropriate depending on the biological question.\n\n\nWith \\(K\\) = 4, you differentiate between genes that are highly expressed under drought but show varying expression under normal conditions. With \\(K\\) = 3, those groups are collapsed. Neither is “wrong” - the choice depends on what you’re trying to understand.\nK-means has been effective for identifying clusters of co-expressed genes from microarray data, where each cluster might correspond to a specific phenotype like height ranges in wheat or cancer subtypes. The algorithm is straightforward and computationally efficient, but it’s sensitive to the initial placement of centroids and your choice of \\(K\\). Many implementations focus on improving the initialization step to achieve more stable convergence and better quality clusters.\nIf you don’t know the number of clusters in advance, or if you want to understand relationships between clusters, hierarchical clustering is a better option."
  },
  {
    "objectID": "posts/unsupervied_learning_intro/index.html#hierarchical-clustering",
    "href": "posts/unsupervied_learning_intro/index.html#hierarchical-clustering",
    "title": "Unsupervised learning: finding patterns without labels",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering groups data points based on pairwise similarity, producing a tree-like structure called a dendrogram that reveals nested relationships between clusters.\nIt can be performed in two ways:\n\nAgglomerative (bottom-up): Each data point starts as its own cluster, and similar clusters merge iteratively until you have one big cluster.\nDivisive (top-down): All data points start in one cluster, which is recursively split into smaller clusters.\n\nThe structure of the dendrogram depends on two key choices: the distance measure and the linkage criterion.\n\nDistance Measures\nEuclidean distance (the formula above) is common for continuous variables, but it’s sensitive to differences in magnitude between features, so you typically need to normalize your data first. Manhattan distance - the sum of absolute differences between corresponding data points - can be more robust to large individual variations. Many specialized distance measures exist depending on the application, particularly for sequence data.\n\n\nLinkage Criteria\nLinkage determines how clusters merge:\n\nSingle linkage: Uses the shortest distance between any two points in different clusters. This tends to form chain-like structures.\nComplete linkage: Uses the largest distance between points in different clusters. This produces more compact clusters.\nAverage linkage: Averages distances between all pairs of points across clusters. This balances the extremes.\n\nYour choice of distance measure and linkage criterion directly affects the dendrogram structure and, ultimately, the biological interpretation of your clusters.\n\n\nApplications in Biology\nHierarchical clustering is widely used in gene expression studies, though it can lack robustness depending on the data. It also underpins many phylogenetic algorithms that infer evolutionary relationships from sequence data.\nPhylogenetic clustering often uses sequence-specific distance measures like alignment scores or substitution models. Common methods include UPGMA (Unweighted Pair Group Method with Arithmetic Mean), Neighbour-Joining, Maximum Parsimony, Maximum Likelihood, and Bayesian inference approaches. The choice of distance metric, alignment method, and model assumptions significantly influences the inferred evolutionary relationships."
  },
  {
    "objectID": "posts/unsupervied_learning_intro/index.html#beyond-clustering",
    "href": "posts/unsupervied_learning_intro/index.html#beyond-clustering",
    "title": "Unsupervised learning: finding patterns without labels",
    "section": "Beyond Clustering",
    "text": "Beyond Clustering\nClustering is just one branch of unsupervised learning. The other major approach - dimensionality reduction - tackles a different problem: how do you visualize or analyze data with hundreds or thousands of features?\nTechniques like Principal Component Analysis (PCA) and t-SNE help simplify high-dimensional datasets by transforming them into lower dimensions while preserving the most meaningful patterns. These methods are particularly useful for exploratory analysis of genomic data, where you might have expression measurements for 20,000 genes but need to understand the overall structure. I’ve put together a full PCA theory and practical tutorial in a seperate blog post."
  },
  {
    "objectID": "posts/unsupervied_learning_intro/index.html#further-reading",
    "href": "posts/unsupervied_learning_intro/index.html#further-reading",
    "title": "Unsupervised learning: finding patterns without labels",
    "section": "Further Reading",
    "text": "Further Reading\nModern Statistics Book Chapter on CLustering"
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html",
    "href": "posts/unsupervised_learning_pca/index.html",
    "title": "A deep dive into PCA for biological data",
    "section": "",
    "text": "The scale of genomics data requires “cutting through the fluff” methods. An RNA-seq experiment might measure expression for 20,000+ genes. A genotyping study might include millions of SNPs across hundreds of samples. You can plot one gene easily, two genes on a 2D scatter plot, three genes in 3D - but what about ten genes? A thousand? Twenty thousand?\nPrincipal Component Analysis (PCA) lets you take multi-dimensional data and produce a 2D plot that shows how samples cluster together and which variables drive those patterns. It’s an unsupervised machine learning method that transforms your original variables (genes, SNPs, metabolites) into a new set of independent variables called principal components.\nPCA aims to: 1. Identify which attributes contribute most to differences between samples 2. Cluster samples into groups based on these attributes 3. Reveal which variables are driving those patterns\nThis tutorial walks through PCA from mathematical intuition to practical implementation in R using SNPRelate - a package designed for genomic data that handles VCF files directly. We’ll build understanding, then apply it to real genotypic data."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#why-dimensionality-reduction-matters",
    "href": "posts/unsupervised_learning_pca/index.html#why-dimensionality-reduction-matters",
    "title": "A deep dive into PCA for biological data",
    "section": "",
    "text": "The scale of genomics data requires “cutting through the fluff” methods. An RNA-seq experiment might measure expression for 20,000+ genes. A genotyping study might include millions of SNPs across hundreds of samples. You can plot one gene easily, two genes on a 2D scatter plot, three genes in 3D - but what about ten genes? A thousand? Twenty thousand?\nPrincipal Component Analysis (PCA) lets you take multi-dimensional data and produce a 2D plot that shows how samples cluster together and which variables drive those patterns. It’s an unsupervised machine learning method that transforms your original variables (genes, SNPs, metabolites) into a new set of independent variables called principal components.\nPCA aims to: 1. Identify which attributes contribute most to differences between samples 2. Cluster samples into groups based on these attributes 3. Reveal which variables are driving those patterns\nThis tutorial walks through PCA from mathematical intuition to practical implementation in R using SNPRelate - a package designed for genomic data that handles VCF files directly. We’ll build understanding, then apply it to real genotypic data."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#the-problem-visualizing-multiple-dimensions",
    "href": "posts/unsupervised_learning_pca/index.html#the-problem-visualizing-multiple-dimensions",
    "title": "A deep dive into PCA for biological data",
    "section": "The Problem: Visualizing Multiple Dimensions",
    "text": "The Problem: Visualizing Multiple Dimensions\nLet’s put this onto a scale that’s easier to illustrate. Suppose you’ve done an RNA-seq experiment measuring gene expression across six samples. If you plot expression for one gene, you can see that samples 1, 2, and 3 cluster together, distinct from samples 4, 5, and 6:\n\n\n\nExpression of a single gene across six samples. Samples 1-3 cluster separately from samples 4-6.\n\n\nFor two genes, you can plot this on a 2D graph:\n\n\n\nExpression of two genes across six samples. Samples 1-3 cluster in the bottom left; samples 4-6 in the top right.\n\n\nEach variable is an axis spanning one dimension. For three genes, you’d need a 3D plot. But what about four genes? Ten? You can’t plot four dimensions, let alone thousands.\nPCA solves this by finding new axes (principal components) that capture the most variance in your data, allowing you to plot high-dimensional data in 2D while preserving meaningful structure."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#how-pca-works-a-visual-walkthrough",
    "href": "posts/unsupervised_learning_pca/index.html#how-pca-works-a-visual-walkthrough",
    "title": "A deep dive into PCA for biological data",
    "section": "How PCA Works: A Visual Walkthrough",
    "text": "How PCA Works: A Visual Walkthrough\nWe’ll use two variables here for visualization, but the principles apply to datasets with thousands of dimensions. You just can’t draw them anymore - but the math still works.\n\nStep 1: Center the Data\nFirst, calculate the mean for each variable. This gives you the center of your data.\n\n\n\nFinding the center of the data by averaging each variable.\n\n\nThen shift the data so this center sits on the origin (0, 0). This normalizes your data to have a mean of 0 and standard deviation of 1. Note that shifting doesn’t change the relative position of points to each other - it just relocates the entire dataset.\n\n\n\nData shifted to center on the origin. This is normalization - mean of 0, standard deviation of 1.\n\n\n\n\nStep 2: Find the Line of Best Fit (PC1)\nNow we need to find the line that best describes the data. There are two equivalent ways to think about this:\n\nMinimize the distance from each data point to the line (the residuals)\nMaximize the distance from projected points on the line to the origin\n\nPCA uses the second approach because it’s computationally easier. But why are these equivalent?\n\n\n\nDemonstrating the equivalence of minimizing residuals and maximizing projected distances using the Pythagorean theorem.\n\n\nConsider one data point, say sample 5. Draw a line from the origin to the sample point (distance \\(a\\)), from the sample point perpendicular to the fitted line (distance \\(b\\)), and from the projected point to the origin (distance \\(c\\)). You’ve formed a right-angled triangle.\nBy the Pythagorean theorem: \\(a^2 = b^2 + c^2\\)\nSince \\(a\\) is fixed (the distance from origin to the sample doesn’t change), maximizing \\(c\\) is the same as minimizing \\(b\\). PCA algorithms maximize \\(c\\) - the distances from projected points to the origin.\n\n\n\nThe sum of squared distances (SSD) from projected points to the origin. When SSD is maximized, we’ve found PC1.\n\n\nThis distance is squared for each point (to remove negative values), then summed. This is the Sum of Squared Distances (SSD). When the SSD is largest, you’ve found the line of best fit. This line is PC1 (Principal Component 1).\nThe average of the SSD (divided by \\(n-1\\), where \\(n\\) is the number of samples) is the eigenvalue for PC1 - a measure of how much variation PC1 captures. The square root of the eigenvalue is the eigenvector or singular value for PC1.\n\n\nStep 3: Interpreting PC1’s Composition\nIf PC1 has a slope of 0.25, it means that for every 4 units along the x-axis (gene 1), you go up 1 unit on the y-axis (gene 2). The data is mostly spread out along the gene 1 axis.\n\n\n\nPC1 with a slope of 0.25 means it’s composed of 4 parts gene 1 and 1 part gene 2.\n\n\nIn other words, PC1 is made up of 4 parts gene 1 and 1 part gene 2. This ratio shows that gene 1 is more important for describing the spread of the data. PC1 captures the linear combination of genes 1 and 2.\nWe can calculate the length of this vector using Pythagoras: \\(\\sqrt{4^2 + 1^2} = \\sqrt{17} = 4.12\\)\nIf you’re using Singular Value Decomposition (SVD) - a common method for computing PCA - the vector is scaled so its length equals 1. Divide each component by the length: \\(4 \\div 4.12 = 0.97\\) and \\(1 \\div 4.12 = 0.24\\).\nThis gives you loading scores: 0.97 parts gene 1 and 0.24 parts gene 2. These scores tell you how much each original variable contributes to PC1.\n\n\nStep 4: Finding PC2\nPC2 is the line through the origin that is perpendicular (orthogonal) to PC1:\n\n\n\nPC2 is orthogonal to PC1 and captures the second-most variance.\n\n\nIf we scale this vector to length 1, we get loading scores of -0.24 parts gene 1 and 0.97 parts gene 2 - the eigenvector for PC2.\nTo draw the final PCA plot, rotate the graph so the projected points fall along the x and y axes (which are now PC1 and PC2). The intersection for each sample gives you the new coordinates to plot."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#how-many-components-should-you-include",
    "href": "posts/unsupervised_learning_pca/index.html#how-many-components-should-you-include",
    "title": "A deep dive into PCA for biological data",
    "section": "How Many Components Should You Include?",
    "text": "How Many Components Should You Include?\nThe eigenvalue for each PC (remember, this is \\(\\text{SSD} / (n-1)\\)) tells you how much variation that PC explains. You can calculate the proportion of total variation each PC accounts for.\nFor example, if PC1’s eigenvalue is 15 and PC2’s is 3: - PC1 accounts for \\(15 / (15 + 3) \\times 100 = 83\\%\\) of total variation - PC2 accounts for \\(3 / 18 \\times 100 = 17\\%\\) of total variation\nYou visualize this with a scree plot - a bar chart showing the percentage of variation each PC explains:\n\n\n\nScree plot showing the proportion of variance explained by each principal component.\n\n\nIf the first two PCs capture most of the variance (say, 70-80%), a 2D PCA plot is a good representation. If variance is spread across many components, you might need a 3D plot or accept that a 2D visualization is incomplete."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#extending-to-higher-dimensions",
    "href": "posts/unsupervised_learning_pca/index.html#extending-to-higher-dimensions",
    "title": "A deep dive into PCA for biological data",
    "section": "Extending to Higher Dimensions",
    "text": "Extending to Higher Dimensions\nEverything we’ve described works the same way for datasets with hundreds or thousands of variables. You can’t visualize it, but the math is identical:\n\nCenter the data (normalize to mean 0, standard deviation 1)\nFind PC1 - the line that maximizes SSD, now a linear combination of all your variables\nFind PC2 - perpendicular to PC1\nFind PC3 - perpendicular to both PC1 and PC2\nContinue until you have one PC per variable (or one per sample, whichever is smaller)\n\nIn practice, you rarely need all the PCs. The first few capture most of the variation, and that’s what you plot."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#summary-what-actually-is-a-principal-component",
    "href": "posts/unsupervised_learning_pca/index.html#summary-what-actually-is-a-principal-component",
    "title": "A deep dive into PCA for biological data",
    "section": "Summary: What Actually Is a Principal Component?",
    "text": "Summary: What Actually Is a Principal Component?\nA principal component is a scaled vector representing the line that best fits the data (maximizing SSD). It describes the linear combination or ratio of variables that account for the spread of the data.\nMultiple PCs can be produced, each weighting the variables differently and independent of the others. The number of PCs equals the number of samples or variables, whichever is lower. PC1 represents the largest sample variation, PC2 the second-largest, and so on.\nPCA plots typically show PC1 vs. PC2 - the two components representing the attributes contributing most to differences between samples. This is best for pattern recognition."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#practical-application-pca-for-genotypic-data-in-r",
    "href": "posts/unsupervised_learning_pca/index.html#practical-application-pca-for-genotypic-data-in-r",
    "title": "A deep dive into PCA for biological data",
    "section": "Practical Application: PCA for Genotypic Data in R",
    "text": "Practical Application: PCA for Genotypic Data in R\nNow let’s apply this to real genomic data. We’ll start with VCF files straight from the sequencer and walk through the entire workflow in R using SNPRelate.\n\nUnderstanding Your Input Data: VCF Files\nWhen you sequence samples and call variants, you typically get a VCF (Variant Call Format) file. This is a tab-delimited text file containing:\n\nMetadata lines (starting with ##) - information about reference genome, filters, sample names\nHeader line (starting with #CHROM) - column names\nVariant records - one line per variant with genotype data for all samples\n\nA simplified example:\n##fileformat=VCFv4.2\n##reference=grape_genome_v2.fa\n#CHROM  POS     ID      REF  ALT  QUAL  FILTER  INFO  FORMAT  Sample1  Sample2  Sample3\nchr1    1000    .       A    T    50    PASS    .     GT      0/0      0/1      1/1\nchr1    2000    .       G    C    55    PASS    .     GT      0/1      0/1      0/0\nThe GT (genotype) field shows alleles: 0/0 = homozygous reference, 0/1 = heterozygous, 1/1 = homozygous alternate.\n\n\nInstalling Required Packages\n# Install Bioconductor packages (if needed)\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"SNPRelate\")\nBiocManager::install(\"gdsfmt\")\n\n# Load packages\nlibrary(SNPRelate)\nlibrary(gdsfmt)\nlibrary(ggplot2)\n\n\nConverting VCF to GDS Format\nSNPRelate works with GDS (Genomic Data Structure) files, which are optimized for large-scale genomic data. First, convert your VCF:\n# Convert VCF to GDS format\nvcf_file &lt;- \"your_samples.vcf.gz\"  # Works with compressed VCF\ngds_file &lt;- \"your_samples.gds\"\n\nsnpgdsVCF2GDS(vcf_file, gds_file, method = \"biallelic.only\")\n\n# Open the GDS file\ngenofile &lt;- snpgdsOpen(gds_file)\n\n# Check what's in the file\nsnpgdsSummary(genofile)\n\n\nQuality Control and Filtering\nBefore PCA, filter your data. SNPRelate provides functions for common QC steps:\n# Get sample and SNP IDs\nsample_ids &lt;- read.gdsn(index.gdsn(genofile, \"sample.id\"))\nsnp_ids &lt;- read.gdsn(index.gdsn(genofile, \"snp.id\"))\n\n# Filter for missing data (keep SNPs with &lt;10% missingness)\nsnp_missing &lt;- snpgdsSNPRateFreq(genofile)\nsnps_to_keep &lt;- snp_ids[snp_missing$MissingRate &lt; 0.1]\n\n# Filter for minor allele frequency (MAF &gt; 0.05)\nsnp_maf &lt;- snpgdsSNPRateFreq(genofile)\nsnps_to_keep &lt;- snps_to_keep[snp_maf$MinorFreq[snp_maf$MissingRate &lt; 0.1] &gt; 0.05]\n\n# Prune for linkage disequilibrium\n# This keeps relatively independent SNPs\nset.seed(1000)\nsnp_pruned &lt;- snpgdsLDpruning(genofile,\n                              ld.threshold = 0.2,\n                              snp.id = snps_to_keep,\n                              method = \"corr\")\n\n# Get the final list of pruned SNPs\nsnps_final &lt;- unlist(snp_pruned)\n\ncat(sprintf(\"Starting with %d SNPs\\n\", length(snp_ids)))\ncat(sprintf(\"After QC: %d SNPs\\n\", length(snps_to_keep)))\ncat(sprintf(\"After LD pruning: %d SNPs\\n\", length(snps_final)))\nWhy these filters? - Missing data: SNPs with high missingness are unreliable - MAF filtering: Rare variants contribute little information and increase noise - LD pruning: If SNPs are inherited together (linkage disequilibrium), they’re not independent. Including them inflates their contribution, and you capture LD structure rather than population structure\nMissing data handling: SNPRelate excludes missing genotypes by default. For advanced imputation (haplotype-based methods), use tools like Beagle or IMPUTE2 before converting to GDS. For most PCA applications, excluding missing data with the filters above is sufficient.\n\n\nComputing Principal Components\nNow run the PCA:\n# Compute PCA with filtered SNPs\npca &lt;- snpgdsPCA(genofile,\n                 snp.id = snps_final,\n                 autosome.only = FALSE,  # Set TRUE for human data\n                 num.thread = 4)\n\n# Check variance explained by each PC\npc_percent &lt;- pca$varprop * 100\ncat(sprintf(\"PC1 explains %.2f%% of variance\\n\", pc_percent[1]))\ncat(sprintf(\"PC2 explains %.2f%% of variance\\n\", pc_percent[2]))\ncat(sprintf(\"First 10 PCs explain %.2f%% of variance\\n\", sum(pc_percent[1:10])))\n\n# Close the GDS file\nsnpgdsClose(genofile)\nThe pca object contains: - pca$eigenvect - PC scores for each sample (coordinates for plotting) - pca$eigenval - eigenvalues (variance explained by each PC) - pca$varprop - proportion of total variance explained by each PC - pca$sample.id - sample IDs in the same order as eigenvectors\n\n\nVisualizing Results\nCreate a comprehensive visualization including scree plot and PCA plot:\n# Prepare data for plotting\npca_data &lt;- data.frame(\n  sample_id = pca$sample.id,\n  PC1 = pca$eigenvect[, 1],\n  PC2 = pca$eigenvect[, 2],\n  PC3 = pca$eigenvect[, 3]\n)\n\n# Add your metadata (population, treatment, etc.)\n# Assumes you have a metadata file with sample info\nmetadata &lt;- read.csv(\"sample_metadata.csv\")\npca_data &lt;- merge(pca_data, metadata, by = \"sample_id\")\n\n# Calculate percentage variance for axis labels\npc1_var &lt;- round(pca$varprop[1] * 100, 2)\npc2_var &lt;- round(pca$varprop[2] * 100, 2)\n\n# Create scree plot\nscree_data &lt;- data.frame(\n  PC = 1:length(pca$eigenval),\n  Variance = pca$varprop * 100\n)\n\nscree_plot &lt;- ggplot(scree_data[1:20, ], aes(x = PC, y = Variance)) +\n  geom_bar(stat = \"identity\", fill = \"#323619\") +\n  geom_text(data = scree_data[1:10, ],\n            aes(label = sprintf(\"%.1f%%\", Variance), y = Variance + 0.5),\n            size = 3, color = \"gray40\") +\n  labs(title = \"Scree plot\",\n       x = \"Principal Component\",\n       y = \"Variance Explained (%)\") +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank())\n\nprint(scree_plot)\n\n# Create PCA plot\npca_plot &lt;- ggplot(pca_data, aes(x = PC1, y = PC2, \n                                  color = population,\n                                  shape = treatment)) +\n  geom_point(size = 3, alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"gray50\") +\n  geom_vline(xintercept = 0, linetype = \"dotted\", color = \"gray50\") +\n  labs(title = \"PCA of Genotypic Data\",\n       x = paste0(\"PC1 (\", pc1_var, \"%)\"),\n       y = paste0(\"PC2 (\", pc2_var, \"%)\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\",\n        panel.grid.minor = element_blank())\n\nprint(pca_plot)\n\n# Optional: 3D visualization\n# BiocManager::install(\"plotly\")\n# library(plotly)\n# plot_ly(pca_data, x = ~PC1, y = ~PC2, z = ~PC3,\n#         color = ~population, symbol = ~treatment,\n#         type = \"scatter3d\", mode = \"markers\")"
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#interpreting-the-pca-plot",
    "href": "posts/unsupervised_learning_pca/index.html#interpreting-the-pca-plot",
    "title": "A deep dive into PCA for biological data",
    "section": "Interpreting the PCA Plot",
    "text": "Interpreting the PCA Plot\nData points: Each point represents an individual sample. These points have been transformed - they’re no longer plotted using the original SNP values. Instead, each sample’s position is calculated as the weighted sum of all SNPs:\n\\[\\text{PC1 value} = \\sum_{i=1}^{n} (\\text{SNP}_i \\text{ value} \\times \\text{SNP}_i \\text{ loading on PC1})\\]\nThe same calculation applies for PC2, PC3, etc.\nPrincipal component axes: PC1 and PC2 represent the two components capturing the largest proportions of genetic variation.\nDistance between points: Reflects genetic similarity. Samples close together are genetically similar; those far apart are dissimilar.\nLoading scores: To see which SNPs contribute most to each PC, examine the loadings:\n# Get loadings for PC1 and PC2\ngenofile &lt;- snpgdsOpen(gds_file)\nsnp_load &lt;- snpgdsPCASNPLoading(pca, genofile)\nsnpgdsClose(genofile)\n\n# Create data frame of loadings\nloading_data &lt;- data.frame(\n  snp_id = snp_load$snp.id,\n  PC1_loading = snp_load$snploading[, 1],\n  PC2_loading = snp_load$snploading[, 2]\n)\n\n# Find SNPs with largest contributions to PC1\ntop_snps_pc1 &lt;- loading_data[order(abs(loading_data$PC1_loading), \n                                    decreasing = TRUE)[1:20], ]\nprint(top_snps_pc1)\nPositive loading scores indicate positive correlation with the PC; negative scores indicate inverse correlation. SNPs with large absolute loadings (positive or negative) drive sample separation."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#choosing-a-pca-tool",
    "href": "posts/unsupervised_learning_pca/index.html#choosing-a-pca-tool",
    "title": "A deep dive into PCA for biological data",
    "section": "Choosing a PCA Tool",
    "text": "Choosing a PCA Tool\nMultiple tools can compute PCs for genomic data. Which should you use?\nSNPRelate (R package): Handles VCF files directly (via GDS conversion) - Memory-efficient for large datasets - Good integration with R’s visualization ecosystem - Works with autosomes and sex chromosomes - Best for: Most genomics applications, especially if you’re already working in R\nPLINK (command-line): - Widely used in human genetics - Fast for standard SNP datasets - Extensive documentation and established workflows - Best for: Established pipelines, very large sample sizes\nEIGENSOFT (SMARTPCA/EIGENSTRAT): - Gold standard for population genetics - Includes outlier detection and statistical significance testing - More sophisticated handling of population structure - Best for: Deep population genetics analysis, when you need Tracy-Widom statistics\nbigsnpr (R package): - Optimized for biobank-scale data - Memory-efficient for massive datasets - Modern R implementation with excellent performance - Best for: UK Biobank-scale analyses\nFor crop genomics and most research applications, SNPRelate is a good choice. It’s handles diverse genomic architectures, and integrates seamlessly with R workflows."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#limitations-and-critical-considerations",
    "href": "posts/unsupervised_learning_pca/index.html#limitations-and-critical-considerations",
    "title": "A deep dive into PCA for biological data",
    "section": "Limitations and Critical Considerations",
    "text": "Limitations and Critical Considerations\nPCA is powerful but not perfect. Key limitations:\n1. Assumes linearity - PCA finds linear combinations of variables. Complex non-linear relationships might be missed.\n2. Prioritizes global variance - Local structure between closely related samples may be obscured.\n3. Sensitive to preprocessing - LD pruning thresholds, MAF cutoffs, and missing data handling all affect results. Different choices give different patterns.\n4. Sampling bias matters - Elhaik (2022) showed that uneven sampling across populations can create artificial clustering. If you sample 100 individuals from population A and 10 from population B, PCA will reflect this imbalance, not necessarily true biological structure.\n5. The “cluster illusion” - PCA plots can suggest discrete groups even in continuously distributed data. Human pattern-seeking sees clusters that may not be biologically meaningful.\n6. LD structure vs. population structure - Improper LD pruning means you capture chromosomal LD patterns rather than genome-wide differentiation.\nBottom line: Use PCA as an exploratory tool. Validate patterns with complementary methods (ADMIXTURE, f-statistics, phylogenetics) before making strong biological claims."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#when-pca-isnt-appropriate-related-samples",
    "href": "posts/unsupervised_learning_pca/index.html#when-pca-isnt-appropriate-related-samples",
    "title": "A deep dive into PCA for biological data",
    "section": "When PCA Isn’t Appropriate: Related Samples",
    "text": "When PCA Isn’t Appropriate: Related Samples\nWhen samples are closely related (full siblings, near-clonal lines, breeding populations), PCA faces challenges: - PCs may capture minor noise rather than meaningful variation - Interpretation becomes difficult - PCs may not correspond to biological differences - Limited variance to explain\nBetter alternatives for related samples: Kinship analysis, mixed linear models, or ADMIXTURE for breeding populations."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#beyond-pca-when-to-use-other-methods",
    "href": "posts/unsupervised_learning_pca/index.html#beyond-pca-when-to-use-other-methods",
    "title": "A deep dive into PCA for biological data",
    "section": "Beyond PCA: When to Use Other Methods",
    "text": "Beyond PCA: When to Use Other Methods\nFor specific biological questions, other dimensionality reduction methods are available:\nt-SNE (t-distributed Stochastic Neighbor Embedding): Preserves local structure and relationships rather than global variance. Popular in single-cell RNA-seq for identifying discrete cell types. Produces well-separated clusters that make cell type identification easier than PCA’s often-overlapping patterns.\nUMAP (Uniform Manifold Approximation and Projection): Similar to t-SNE but faster and often better at preserving both local and global structure. Sometimes preferred in modern single-cell analysis.\nSelf-Organizing Maps (SOMs/Kohonen maps): Unsupervised neural networks creating 2D grid representations. Useful for pattern recognition without labels. Tutorial here.\nWhen to use what: - PCA: Most genomics applications - population structure, GWAS, expression analysis. Fast, interpretable, deterministic. - t-SNE/UMAP: Single-cell data, discrete cluster identification, exploratory analysis where local relationships matter more than global structure. - PCoA: Working with custom distance metrics or phylogenetic distances (analyzes pairwise dissimilarity matrices rather than raw data).\nFor population genetics and crop genomics, PCA remains the standard. Its interpretability (you can trace patterns back to specific SNPs), speed, and deterministic results make it the right tool for most jobs."
  },
  {
    "objectID": "posts/unsupervised_learning_pca/index.html#further-resources",
    "href": "posts/unsupervised_learning_pca/index.html#further-resources",
    "title": "A deep dive into PCA for biological data",
    "section": "Further Resources",
    "text": "Further Resources\nR Packages and Documentation: - SNPRelate documentation - SNPRelate tutorial - gdsfmt package\nTutorials: - PCA in R (DataCamp) - PCA in Python - from Maria Nattestad, who explain this very well\nCritical perspective:\n- Elhaik (2022). “Principal component analyses (PCA)-based findings in population genetic studies are highly biased and must be reevaluated.” Scientific Reports 12:14683\n\nClosing thoughts: PCA is a high-dimensional data visualization technique, but its usefulness depends on whether the first few principal components capture most variation. Check your scree plot - if PC1 and PC2 together explain &lt;50% of variance, consider whether a 2D visualization is truly representative. And remember: PCA is exploratory. It suggests patterns, but you need complementary analyses to confirm biological meaning."
  },
  {
    "objectID": "posts/supervised_learning/index.html",
    "href": "posts/supervised_learning/index.html",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "",
    "text": "In a previous post, we looked at neural networks from a high level - what they are, why they’re useful for biology, and how they fit into the broader machine learning landscape. We talked about perceptrons as computational units that learn to recognise patterns by adjusting weights.\nThen we explored unsupervised learning - clustering methods that find structure in unlabelled data. K-means groups samples by similarity. Hierarchical clustering builds dendrograms showing nested relationships. Both are exploratory tools for when you don’t know what patterns exist.\nNow let’s look at the other major branch: supervised learning. This is where you have labels. You know the answers. And you want an algorithm to learn the mapping from inputs to outputs so it can predict labels for new, unseen data.\nThis post unpacks the mathematics. How do these algorithms actually learn? What’s happening under the hood when we “train a model”? And what are the common failure modes to avoid?"
  },
  {
    "objectID": "posts/supervised_learning/index.html#where-weve-been-where-were-going",
    "href": "posts/supervised_learning/index.html#where-weve-been-where-were-going",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "",
    "text": "In a previous post, we looked at neural networks from a high level - what they are, why they’re useful for biology, and how they fit into the broader machine learning landscape. We talked about perceptrons as computational units that learn to recognise patterns by adjusting weights.\nThen we explored unsupervised learning - clustering methods that find structure in unlabelled data. K-means groups samples by similarity. Hierarchical clustering builds dendrograms showing nested relationships. Both are exploratory tools for when you don’t know what patterns exist.\nNow let’s look at the other major branch: supervised learning. This is where you have labels. You know the answers. And you want an algorithm to learn the mapping from inputs to outputs so it can predict labels for new, unseen data.\nThis post unpacks the mathematics. How do these algorithms actually learn? What’s happening under the hood when we “train a model”? And what are the common failure modes to avoid?"
  },
  {
    "objectID": "posts/supervised_learning/index.html#the-fundamental-idea",
    "href": "posts/supervised_learning/index.html#the-fundamental-idea",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "The fundamental idea",
    "text": "The fundamental idea\nSupervised learning means teaching an algorithm by example. You give your learning algorithm examples to learn from that include the correct answer or label (\\(y\\)) for every input feature (\\(x\\)), so that eventually you can just give an unlabelled input and the model will give a reasonably accurate prediction.\nThe more accurate this prediction, and the more unseen datasets you can apply the model to without sacrificing accuracy, the better the model overall.\n\n\n\n\n\n\nNoteTerminology\n\n\n\n\nModel: A mathematical algorithm (often executed via a function or program), with variable parameters that have been optimised (the model has “learned” these values through training datasets) so that when applied to new input data, it can make reasonable output predictions\nx (features): These are the input variables or features used to make predictions. They represent the independent variables that influence the outcome\ny (target/labels): This is the variable we want to predict or classify. It represents the dependent variable (generally the last column of your dataframe)\nm: Often used as the standard symbol for the number of training examples in the dataset (rows/samples/observations)\nn: Similarly, n is used to denote the number of features (columns/variables/markers)\n\\((x^{(i)}, y^{(i)})\\): To refer to a single training example / sample / row of data\n\\(x_j\\): To refer to a single feature / trait / marker / column\n\\(\\hat{y}\\): Referred to as “y-hat”, this represents a prediction of the output \\(y\\) based on the function or model \\(f()\\)\n\n\n\nTwo main types:\nRegression: Predict a continuous numerical value. Gene expression levels in TPM. Growth rate in mm/day. Drought tolerance on a 0-100 scale.\nClassification: Predict which category something belongs to. Disease-resistant or not. Cell type A, B, or C. High-risk or low-risk.\nThe better the predictions on unseen data, the better the model. Simple enough concept. Now let’s see how it actually works."
  },
  {
    "objectID": "posts/supervised_learning/index.html#starting-simple-linear-regression",
    "href": "posts/supervised_learning/index.html#starting-simple-linear-regression",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "Starting simple: linear regression",
    "text": "Starting simple: linear regression\nThe most intuitive place to start is with a straight line. Suppose you’ve measured expression for one gene across many grapevine samples, and you think this gene correlates with sugar content in the berries. Plot gene expression on the x-axis, sugar content on the y-axis, and you see a pattern - higher expression, higher sugar.\n(This is an arbitrary example for illustration - nothing in biology is quite this simple, and gene expression rarely maps linearly to phenotype. But it gives us an intuitive starting point.)\nA linear model takes the form:\n\\[f_{w,b}(x) = wx + b\\]\nWhere: - \\(w\\) is the weight (the slope of the line) - \\(b\\) is the bias (the y-intercept) - \\(x\\) is your input feature (gene expression) - \\(f_{w,b}(x)\\) is the predicted output (sugar content)\nThis is univariate linear regression - one input feature. The goal is to find the values of \\(w\\) and \\(b\\) that make the line fit your data as closely as possible.\n\n\n\nA simple linear model illustrating the linear regression function \\(f_{w,b}(x^{(i)}) = wx^{(i)} + b\\) and the difference or error (\\(L\\)) shown with the dotted line between the actual target value \\(y^{(i)}\\) (green) and the model predicted value \\(\\hat{y}^{(i)}\\).\n\n\nFor any given sample \\(i\\), the predicted value is \\(\\hat{y}^{(i)} = f_{w,b}(x^{(i)})\\). The actual measured value is \\(y^{(i)}\\). The difference between them is the error."
  },
  {
    "objectID": "posts/supervised_learning/index.html#measuring-how-wrong-you-are-the-cost-function",
    "href": "posts/supervised_learning/index.html#measuring-how-wrong-you-are-the-cost-function",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "Measuring how wrong you are: the cost function",
    "text": "Measuring how wrong you are: the cost function\nOnce you’ve chosen values for \\(w\\) and \\(b\\), you need to know how well they work. Enter the cost function.\nThe error or loss for a single data point is:\n\\[L = \\hat{y}^{(i)} - y^{(i)}\\]\nOr equivalently:\n\\[L = f_{w,b}(x^{(i)}) - y^{(i)}\\]\nBut you don’t just care about one data point. You want a single value that represents how the model is performing over the entire dataset - all \\(m\\) samples. So we take the error for each example in the dataset and get a kind of average. This is exactly what the cost function does:\n\\[J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\]\nThis is the mean squared error (MSE) cost function. A few things to note:\nWhy square the error? Two reasons. First, it removes negative values - errors above and below the line don’t cancel out. Second, it penalises larger errors more heavily. If you’re off by 10 instead of 1, the squared error is 100 times worse, not 10 times worse.\nWhy divide by \\(2m\\) instead of \\(m\\)? The \\(1/m\\) gives you the mean. The extra factor of \\(1/2\\) doesn’t change the optimisation (it’s just a constant), but it simplifies the derivative later when we do gradient descent. It’s a mathematical convenience.\nThe goal is clear: minimise \\(J(w,b)\\). Find the parameters where the cost is as small as possible."
  },
  {
    "objectID": "posts/supervised_learning/index.html#finding-the-minimum-gradient-descent",
    "href": "posts/supervised_learning/index.html#finding-the-minimum-gradient-descent",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "Finding the minimum: gradient descent",
    "text": "Finding the minimum: gradient descent\nSo how do you find the minimum of the cost function? You could try plotting \\(J(w,b)\\) for every possible combination of \\(w\\) and \\(b\\), but that’s computationally ridiculous for anything beyond toy examples.\nInstead, we use gradient descent.\n\n\n\nA plot of the cost function \\(J(w,b)\\) for varying values of \\(w\\) and \\(b\\) within the range -3 to 3.\n\n\nImagine you’re standing on this surface somewhere random. You want to get to the lowest point. Gradient descent is simple: look around, figure out which direction is steepest downhill, and take a step in that direction. Repeat until you reach the bottom.\nIn essence, starting from any randomly chosen position for \\(w\\) and \\(b\\), the algorithm takes incremental and iterative steps in whichever direction the gradient is steepest - it finds the way to get to the bottom of the slope (the minimum) as efficiently as possible.\nMathematically:\n\\[w = w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}\\]\n\\[b = b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}\\]\nWhere: - \\(\\frac{\\partial J(w, b)}{\\partial w}\\) is the derivative of the cost function with respect to \\(w\\) - it tells you the steepness of the slope - \\(\\alpha\\) is the learning rate - it controls how big each step is - You subtract because you want to move down the slope\nThe values for the parameters \\(w\\) and \\(b\\) are repeatedly and simultaneously updated until the minimum is found, also known as convergence. Convergence happens when the parameter values are no longer significantly changing.\nThe learning rate matters. Too small, and convergence is painfully slow. Too large, and you overshoot the minimum, bouncing around without ever settling.\nFor simple linear regression, the cost function is convex - it has a single, bowl-shaped minimum. Gradient descent is guaranteed to find it eventually (assuming a reasonable learning rate). For more complex models, the landscape has multiple local minima, and where you start affects where you end up."
  },
  {
    "objectID": "posts/supervised_learning/index.html#scaling-up-multiple-features",
    "href": "posts/supervised_learning/index.html#scaling-up-multiple-features",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "Scaling up: multiple features",
    "text": "Scaling up: multiple features\nOne gene is rarely the whole story. You usually have multiple features - expression levels for several genes, or multiple SNP markers, or a combination of genomic and environmental measurements.\nThe model extends naturally to multiple linear regression:\n\\[f_{\\vec{w},b}(\\vec{x}) = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\\]\nNow \\(\\vec{x}\\) is a vector of features, and \\(\\vec{w}\\) is a vector of weights. You can also write this as the dot product:\n\\[f_{\\vec{w},b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b\\]\nThe cost function looks the same, just vectorised:\n\\[J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})^2\\]\nAnd gradient descent updates each weight independently:\n\\[w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(\\vec{w}, b)\\]\nThe logic is identical. More features means more weights to optimise, but the underlying principle - minimise the squared error by following the gradient - stays the same.\nThe model can also extend to higher-order polynomials. If the relationship isn’t a straight line, you might have terms like \\(x^2\\) or \\(x^3\\) or interactions between features. The cost function remains MSE, but the function \\(f_{\\vec{w},b}(\\vec{x})\\) can now represent some higher-order equation, and the derivatives of the cost function with respect to these parameters may be more complex."
  },
  {
    "objectID": "posts/supervised_learning/index.html#beyond-straight-lines-classification-with-logistic-regression",
    "href": "posts/supervised_learning/index.html#beyond-straight-lines-classification-with-logistic-regression",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "Beyond straight lines: classification with logistic regression",
    "text": "Beyond straight lines: classification with logistic regression\nPredicting continuous values is useful, but biology often deals with categories. Is this plant drought-resistant or not? Does this sample belong to population A or population B? Is this mutation pathogenic?\nMany biological questions involve classification of some kind, and linear regression models are not suitable for solving classification tasks. Enter logistic regression - despite the name, it’s a classification algorithm.\nThe core problem: linear regression outputs any real number. But for binary classification, you want a probability between 0 and 1. The solution is the sigmoid function:\n\\[g(z) = \\frac{1}{1 + e^{-z}}\\]\n\n\n\nA graph of the sigmoid curve showing the output always between 0 and 1.\n\n\nThe sigmoid takes any input \\(z\\) and squashes it between 0 and 1. The tails approach 0 and 1 asymptotically but never quite reach them.\nFor logistic regression, \\(z\\) is your linear model:\n\\[z = \\vec{w} \\cdot \\vec{x} + b\\]\nSo the full prediction function becomes:\n\\[g(\\vec{w} \\cdot \\vec{x} + b) = \\frac{1}{1 + e^{-(\\vec{w} \\cdot \\vec{x} + b)}}\\]\nThe output is the probability that the sample belongs to the positive class. If the probability is above some threshold (typically 0.5), you classify it as positive. Below the threshold, negative.\nThe decision boundary is where the probability equals 0.5. If the decision boundary is linear (a straight line accurately divides the data into two categories), the value of \\(z\\) takes on the linear regression expression. However, if the decision boundary is best described by some other function or more complex polynomial, then \\(z\\) will change accordingly. In this way, the sigmoid function can accommodate complex data patterns with decision boundaries that are non-linear.\n\nA different cost function for classification\nMean squared error doesn’t work well for logistic regression. The problem is that the sigmoid function makes the cost landscape non-convex - lots of local minima, and gradient descent gets stuck.\nInstead, we use log loss (also called binary cross-entropy):\n\\[J(\\vec{w}, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(g(z^{(i)})) + (1 - y^{(i)}) \\log(1 - g(z^{(i)})) \\right]\\]\nThis looks intimidating, but the logic is straightforward:\n\nIf \\(y = 1\\) (true label is positive), the cost simplifies to \\(-\\log(g(z))\\)\nIf \\(y = 0\\) (true label is negative), the cost simplifies to \\(-\\log(1 - g(z))\\)\n\nThe effect: if the model predicts high probability for the correct class, the loss is small. If it predicts high probability for the wrong class, the loss is large. The model is penalised more when it’s confidently wrong.\nLog loss is convex for logistic regression, so gradient descent works reliably."
  },
  {
    "objectID": "posts/supervised_learning/index.html#the-two-ditches-overfitting-and-bias",
    "href": "posts/supervised_learning/index.html#the-two-ditches-overfitting-and-bias",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "The two ditches: overfitting and bias",
    "text": "The two ditches: overfitting and bias\nAs models get more complex - more features, higher-order polynomials, more parameters - two failure modes emerge.\nOverfitting happens when the model fits the training data too precisely. It captures not just the underlying pattern but also the noise and random fluctuations. Training accuracy looks great. But the model doesn’t generalise - performance on new data is poor. Overfit models have high variance due to their sensitivity to noise.\nBias (or underfitting) is the opposite. The model is too simple to capture the real pattern. It misses the signal entirely. Both training and test accuracy are mediocre. Another way to think of bias is in the English sense of the word - a strong preconception that prevents the model from predicting the true values.\n\n\n\nAn illustration of models that underfit (left) with high bias and oversimplified expression; overfit (right) with high variance and oversensitive polynomial expression; and generalise well (middle) with a model that captures the underlying pattern.\n\n\nA model that generalises well avoids both extremes. It captures the underlying pattern without being distracted by noise.\nHow do you avoid these ditches?\n\nFeature selection\nIf you have many features but few training samples, overfitting is likely. The model has too much freedom and not enough constraints. Overfitting can occur when there are many features in a model, but insufficient training examples. This could be solved by either collecting more data or, alternatively, by selecting a subset of features.\nFeature selection involves choosing the most relevant variables in a dataset, which can significantly improve model accuracy while reducing computational costs and training time. Irrelevant features do not correlate with target labels and can add noise, increasing model complexity without improving predictive performance. Redundancy, or multicollinearity, occurs when multiple features that are correlated with the target labels are also highly correlated with each other, which can lead to overfitting.\nIdentifying the most relevant features in high-dimensional datasets is often challenging. In some cases, domain expertise can help guide feature selection, particularly when the dataset is small or well understood. However, if little is known about the biology of the features or if the dataset is high-dimensional, then reduction techniques such as PCA can be useful tools to inform these selections.\nEnsemble methods like random forests or boosting combine multiple models trained on different subsets of features, reducing overfitting by averaging predictions.\nFeature scaling or transformation - normalising features to a common range or standardising them (mean 0, standard deviation 1) - often improves model performance, especially for algorithms sensitive to feature magnitude.\n\n\nRegularisation\nSometimes removing features entirely is too drastic. Regularisation offers a gentler approach - it penalises large weights instead of eliminating them.\nThe regularised cost function adds a penalty term:\n\\[J_{\\text{reg}}(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\\]\nThe new term \\(\\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\\) penalises large weights. The hyperparameter \\(\\lambda\\) controls the strength of the penalty.\n\nIf \\(\\lambda\\) is too large, the model over-smooths and introduces bias\nIf \\(\\lambda\\) is too small (close to 0), regularisation has no effect\n\nTuning the value of \\(\\lambda\\) is an important hyperparameter optimisation step to balance model complexity and generalisation.\nThis is L2 regularisation (or Ridge regression). The penalty is the sum of squared weights.\nL1 regularisation (or Lasso) uses the absolute value of weights instead: \\(\\sum |w_j|\\). This can shrink some weights all the way to zero, effectively performing feature selection.\nElastic net combines L1 and L2 penalties.\nRegularisation works best when features are scaled, so the penalty isn’t skewed by large ranges."
  },
  {
    "objectID": "posts/supervised_learning/index.html#tying-it-back-to-neural-networks",
    "href": "posts/supervised_learning/index.html#tying-it-back-to-neural-networks",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "Tying it back to neural networks",
    "text": "Tying it back to neural networks\nRemember the neural networks post where we talked about perceptrons? Each perceptron calculates a weighted sum of inputs, applies a threshold, and passes the signal forward if the threshold is exceeded.\nThat weighted sum? It’s \\(\\vec{w} \\cdot \\vec{x} + b\\) - exactly the linear model we’ve been discussing.\nThe training process? Gradient descent minimising a cost function - exactly what we’ve just unpacked.\nThe difference is scale. A neural network is a collection of these units arranged in layers, with non-linear activation functions allowing the network to learn complex patterns. But the underlying mathematics - the cost function, the gradient descent, the weight updates - are the same principles we’ve covered here.\nDeep learning isn’t magic. It’s supervised learning with many layers. The maths scales, but the logic remains."
  },
  {
    "objectID": "posts/supervised_learning/index.html#the-workflow-in-brief",
    "href": "posts/supervised_learning/index.html#the-workflow-in-brief",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "The workflow in brief",
    "text": "The workflow in brief\nHere’s what supervised learning looks like in practice:\n\nCollect and preprocess data - quality control, handle missing values, format correctly\nFeature engineering - select relevant features, transform or encode them appropriately (e.g., one-hot encoding for DNA sequences where A = [1,0,0,0], T = [0,1,0,0], etc.)\nSplit the dataset - typically 60-70% training, 20-30% test, optionally 10-20% validation for hyperparameter tuning\nTrain the model - optimise parameters by minimising the cost function via gradient descent\nEvaluate performance - check training error and test error to assess overfitting, bias, or generalisation\nRefine and iterate - tune hyperparameters, adjust features, try regularisation\n\n\nEvaluating model performance with metrics\nModel performance evaluation is performed with error analysis - comparing the fraction of the training and test datasets that the model has misclassified after training. If the model has high variance and is overfitting, the training error may be minimal, while the test error remains high. If there is bias, then both error values are likely to be high. When both error values are low, or as low as possible, then the model is generalising well and can be applied to unseen datasets. This “low-enough” error value can be decided on by setting a baseline level or benchmark for model performance, by looking to experimental studies, previous models generated on similar datasets, or an industry standard.\nBesides error analysis, several metrics are commonly used to evaluate classification model performance:\nPrecision and recall: Precision refers to the accuracy of positive predictions - what proportion of positive predictions are true positives? (calculated as: true positives / (true positives + false positives)). Recall measures the sensitivity to positive predictions - what proportion of actual positives did we correctly identify? (calculated as: true positives / (true positives + false negatives)).\nF1 score: This metric combines both precision and recall into one value using the harmonic mean: \\(F1 = 2 \\times \\frac{P \\times R}{P + R}\\). Ideally, we want to balance high precision (high confidence in a positive prediction) with high recall (not missing too many true positives), since an inverse relationship often exists between them. When F1 is close to 1, there is balance between these values, but when it is close to 0, one of these metrics is too low.\nAUROC: For imbalanced class distributions (where you have more data for some categories than others), the AUROC (area under the receiver operating characteristic curve) measure may be useful. This technique plots true positive rate against false positive rate at varying classification thresholds. The area under this curve can be interpreted as model performance, ranging from 0 to 1, with values closer to 1 indicating better performance.\nCross-validation: To further improve a model, the validation dataset is useful. Firstly, testing the data on different splits, instead of a single training/test split, gives a more robust measure of performance. After initial training, the validation set can be used to iteratively check the model performance, and to rotate the training and validation sets as many times as required to tune the hyperparameters in a process called cross-validation."
  },
  {
    "objectID": "posts/supervised_learning/index.html#whats-next",
    "href": "posts/supervised_learning/index.html#whats-next",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "What’s next",
    "text": "What’s next\nThis covered the theory - the mathematics behind how supervised learning actually works. Future posts will dig into practical implementation (how to actually code these models), specific algorithms (support vector machines, decision trees, random forests), and biological applications.\nFor now, the key takeaway: supervised learning is about finding a function that maps inputs to outputs by minimising a cost function. Gradient descent is the workhorse algorithm that does the optimisation. The challenge is balancing complexity (capture the pattern) with simplicity (don’t overfit the noise).\nAnd those same principles underpin everything from linear regression to deep neural networks."
  },
  {
    "objectID": "posts/supervised_learning/index.html#further-reading",
    "href": "posts/supervised_learning/index.html#further-reading",
    "title": "Supervised learning: teaching algorithms to learn from example",
    "section": "Further reading",
    "text": "Further reading\n\n3Blue1Brown’s gradient descent visualisation - exceptional visual explanations\nAn Introduction to Statistical Learning - comprehensive, accessible textbook\nDeep Learning with Python (3rd edition) by François Chollet - excellent practical guide\nAndrew Ng’s Deep Learning Specialisation on Coursera - this course was a major inspiration for this post and provided the foundational understanding for many of these concepts"
  },
  {
    "objectID": "posts/chpc_wiki/index.html",
    "href": "posts/chpc_wiki/index.html",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "",
    "text": "This guide is specifically for researchers using the Centre for High Performance Computing (CHPC) in South Africa. If you’re not affiliated with a South African research institution or don’t have CHPC access, this won’t be directly relevant to you - though the concepts apply broadly to other HPCs.\nThis is written for fellow students and colleagues who’ve asked me how to get started, particularly if you’re new to the command line or have never used an HPC before. The CHPC wiki is comprehensive and well-written, but if you’re doing bioinformatics work, you don’t need all of it. This guide extracts the essentials to get you from zero to running your first job.\nWhat this covers: Logging in, understanding the system, running interactive sessions, submitting jobs, and transferring files.\nWhat this doesn’t cover (yet): Singularity containers, Conda environments, advanced scheduling, or specific bioinformatics pipelines. Those will come in future posts."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#who-this-is-for",
    "href": "posts/chpc_wiki/index.html#who-this-is-for",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "",
    "text": "This guide is specifically for researchers using the Centre for High Performance Computing (CHPC) in South Africa. If you’re not affiliated with a South African research institution or don’t have CHPC access, this won’t be directly relevant to you - though the concepts apply broadly to other HPCs.\nThis is written for fellow students and colleagues who’ve asked me how to get started, particularly if you’re new to the command line or have never used an HPC before. The CHPC wiki is comprehensive and well-written, but if you’re doing bioinformatics work, you don’t need all of it. This guide extracts the essentials to get you from zero to running your first job.\nWhat this covers: Logging in, understanding the system, running interactive sessions, submitting jobs, and transferring files.\nWhat this doesn’t cover (yet): Singularity containers, Conda environments, advanced scheduling, or specific bioinformatics pipelines. Those will come in future posts."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#what-is-an-hpc-and-why-do-you-need-one",
    "href": "posts/chpc_wiki/index.html#what-is-an-hpc-and-why-do-you-need-one",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "What Is an HPC and Why Do You Need One?",
    "text": "What Is an HPC and Why Do You Need One?\nHigh Performance Computing uses clusters of processors working in parallel to process massive datasets. Your laptop solves problems serially - it divides work into sequential tasks and executes them one after another. An HPC runs multiple tasks simultaneously across many processors.\nFor bioinformatics, this matters because: - large tasks can take days or weeks on a single machine - for instance, mapping millions of reads requires substantial memory - and running the same analysis across 100+ samples is tedious without parallelization\nThe CHPC gives you access to thousands of CPU cores, hundreds of gigabytes of RAM, and GPUs - resources you can’t get on a standard machine.\n\nKey Terminology\nBefore we start, some quick definitions:\nNode: A physical machine (server) in the cluster. Think of it as an individual computer. The CHPC has thousands of nodes.\nCore: The physical processing unit of a CPU. More cores = more parallel processing. A CPU with 24 cores can run 24 independent processes simultaneously.\nThread: A logical or virtual core. Threading allows a single core to work on two tasks by switching between them rapidly. Many bioinformatics tools let you specify --threads 8, but actual speedup depends on the number of physical cores.\nJob: A task you submit to the cluster - a script, a command, an analysis pipeline.\nQueue: A holding area for jobs waiting for resources. Jobs are scheduled based on resource requests and availability.\nWalltime: The maximum time your job is allowed to run. If it exceeds this, it’s killed."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#about-the-chpc",
    "href": "posts/chpc_wiki/index.html#about-the-chpc",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "About the CHPC",
    "text": "About the CHPC\nThe Centre for High Performance Computing is part of the National Integrated Cyberinfrastructure System (NICIS), supported by the Department of Science and Innovation and the CSIR. You access it either as a Principal Investigator (PI) or as a Normal User under a research program.\nKey links: - CHPC Quick Start Guide - the central hub for everything - User Portal - for account management and support tickets - Full Wiki - comprehensive documentation\nIf you’re working with a supervisor or lab that already has access, they’ll register you under their project code. You’ll get an account with a username and password."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#logging-in",
    "href": "posts/chpc_wiki/index.html#logging-in",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "Logging In",
    "text": "Logging In\nThe CHPC has multiple login nodes. You’ll primarily use two:\nlogin1 (scp node): - Purpose: File transfers (secure copy) - Login: ssh username@scp.chpc.ac.za - Use for: Uploading/downloading data\nlogin2 (lengau node): - Purpose: Primary shared login node (default) - Login: ssh username@lengau.chpc.ac.za - Use for: Running commands, submitting jobs, interactive sessions\nImportant: Because lengau is shared and resource-intensive, interactive commands are killed unless you request an interactive session (more on this below). You can’t just run computationally heavy tasks directly on the login node.\n\nSwitching Between Nodes\nOnce logged in, you can switch between nodes:\nssh login1      # Switch to scp node\nexit            # Return to lengau\nNeither login node has internet access. If you need to download data from online databases (e.g., NCBI SRA), use chpclic1:\nssh username@chpclic1.chpc.ac.za\nThis node has internet access and is specifically for data downloads."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#understanding-the-file-system",
    "href": "posts/chpc_wiki/index.html#understanding-the-file-system",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "Understanding the File System",
    "text": "Understanding the File System\nThis is critical to get right from the start.\nWhen you log in, you’ll be in your home directory: /home/username\nDon’t work here. Your home directory has a 15GB limit and isn’t designed for analysis work.\nInstead, work in your Lustre directory: /mnt/lustre/users/username/\nThis is your primary working space. It has much more storage and is where you should keep all project data, scripts, and outputs.\nImportant caveat: The CHPC cleans out Lustre files every 3 months. Back up your analyses and download results every 90 days.\n\nSetting Up a Shortcut\nTyping /mnt/lustre/users/username/ every time is tedious. Create a shortcut:\ncd /home/username\nnano .bashrc\n\n# Add this line (replace with your actual path):\nalias wkd=\"cd /mnt/lustre/users/username/\"\n\n# Save (Ctrl+O, Enter) and exit (Ctrl+X)\nNow source the file to activate it:\nsource ~/.bashrc\nFrom now on, just type wkd to jump straight to your working directory.\n\n\nChecking Disk Usage\nTo see how much space you’re using:\ndu --si -s $HOME"
  },
  {
    "objectID": "posts/chpc_wiki/index.html#organizing-your-projects",
    "href": "posts/chpc_wiki/index.html#organizing-your-projects",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "Organizing Your Projects",
    "text": "Organizing Your Projects\nCreate a clear directory structure for each project. I use:\n/mnt/lustre/users/username/project_name/\n├── raw/          # Raw sequencing data\n├── bin/          # Scripts\n├── out/          # Analysis outputs\n│   ├── assembly/\n│   ├── mapping/\n│   └── qc/\n└── ref/          # Reference genomes, annotations\nKeep everything organized from the start. Future you will thank present you."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#interactive-sessions-testing-and-small-jobs",
    "href": "posts/chpc_wiki/index.html#interactive-sessions-testing-and-small-jobs",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "Interactive Sessions: Testing and Small Jobs",
    "text": "Interactive Sessions: Testing and Small Jobs\nInteractive sessions let you work directly on a compute node rather than the login node. This is essential for: - Testing commands before writing a full script - Running quick analyses - Debugging pipelines\n\nRequesting an Interactive Session\nqsub -I -l select=1:ncpus=4:mpiprocs=4 -q serial -P CBBI1684 -l walltime=1:00:00\nBreaking this down: - -I: Interactive session - -l select=1: Request 1 node - ncpus=4: Use 4 CPU cores - mpiprocs=4: 4 MPI processes per node - -q serial: Use the serial queue - -P CBBI1684: Your project code (replace with yours) - -l walltime=1:00:00: Request 1 hour\nOnce allocated, you’ll be dropped into a compute node where you can run commands directly.\nCaveat: If your terminal is idle for ~5 minutes, you’ll be kicked out and your job is killed. If you need to step away, use screen.\n\n\nUsing screen for Persistent Sessions\nscreen lets you detach from a session and reconnect later without stopping your work:\nStart a screen session:\nscreen -S mysession\nDetach (leave it running): Press Ctrl+A, then D\nYou’ll drop back to the terminal, but your session continues in the background.\nList active sessions:\nscreen -ls\nReattach to a session:\nscreen -r mysession\nKill a session: From inside: exit From outside: screen -S mysession -X quit\nNote: HPCs prefer you submit actual jobs through the scheduler rather than running long interactive sessions. Use screen for quick development and testing, but submit proper jobs for anything that takes more than an hour."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#loading-software-the-module-system",
    "href": "posts/chpc_wiki/index.html#loading-software-the-module-system",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "Loading Software: The Module System",
    "text": "Loading Software: The Module System\nThe CHPC has a huge repository of pre-installed bioinformatics software. To use it, you need to load the module system.\n\nLoading BIOMODULES\nmodule load chpc/BIOMODULES\nThis gives you access to all bioinformatics packages available on the cluster.\n\n\nFinding Available Software\nList all available modules:\nmodule avail\nThis produces a long list. To search more effectively:\n# Search with less (use / to search, q to quit)\nmodule avail 2&gt;&1 | less\n\n# Search with grep\nmodule avail 2&gt;&1 | grep fastqc\nmodule avail 2&gt;&1 | grep spades\n\n\nLoading a Module\nOnce you’ve found what you need:\nmodule load fastqc/0.11.9\n\n\nChecking Loaded Modules\nmodule list\n\n\nUnloading a Module\nmodule unload fastqc/0.11.9\nIf a tool you need isn’t available, submit a ticket through the CHPC helpdesk, or install it in your own directory if you have space."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#submitting-jobs-the-pbs-scheduler",
    "href": "posts/chpc_wiki/index.html#submitting-jobs-the-pbs-scheduler",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "Submitting Jobs: The PBS Scheduler",
    "text": "Submitting Jobs: The PBS Scheduler\nFor any substantial analysis, you’ll submit a job to the scheduler rather than running it interactively. The CHPC uses PBS Pro as its job scheduler.\n\nAvailable Queues\nDifferent queues have different resource limits and policies. The CHPC wiki has a full table, but for most bioinformatics work, you’ll use:\n\nserial: Default queue, up to 24 cores per node, 48-hour walltime\nsmp: Shared memory processing, more walltime available\nnormal: For larger multi-node jobs\n\n\n\nWriting a PBS Script\nCreate a script in your bin/ directory:\ncd /mnt/lustre/users/username/project_name/bin/\nnano fastqc_job.sh\nHere’s a template PBS script:\n#!/bin/bash\n#PBS -N fastqc_job              # Job name\n#PBS -q serial                  # Queue to submit to\n#PBS -P CBBI1684                # Project code (replace with yours)\n#PBS -l select=1:ncpus=24       # 1 node, 24 cores\n#PBS -l walltime=48:00:00       # Max runtime (48 hours)\n#PBS -e /mnt/lustre/users/username/project/logs/fastqc.err\n#PBS -o /mnt/lustre/users/username/project/logs/fastqc.out\n\n# Load required modules\nmodule load chpc/BIOMODULES\nmodule load fastqc/0.11.9\nmodule load multiqc/1.9\n\n# Define paths\nRAW_DATA=/mnt/lustre/users/username/project/raw\nQC_OUT=/mnt/lustre/users/username/project/out/qc\n\n# Run FastQC on all fastq files\nfastqc -t 12 ${RAW_DATA}/*.fastq.gz -o ${QC_OUT}\n\n# Aggregate results with MultiQC\nmultiqc ${QC_OUT} -o ${QC_OUT}\nWhat’s happening: 1. PBS directives (lines starting with #PBS) set job parameters 2. Modules are loaded to access software 3. Paths are defined for clarity 4. FastQC runs on 12 threads across all fastq files 5. MultiQC aggregates the results into a single report\n\n\nSubmitting the Job\nqsub fastqc_job.sh\nYou’ll get a job ID back, something like 123456.sched.\n\n\nChecking Job Status\n# Check all your jobs\nqstat -u username\n\n# Check a specific job\nqstat 123456.sched\nThe S column shows status: - Q: Queued (waiting for resources) - R: Running - C: Completed\n\n\nCancelling a Job\nqdel 123456.sched\n\n\nChecking Output and Errors\nYour error and output files (specified in the script with #PBS -e and #PBS -o) will contain any messages or errors from the job. Always check the error log first if something goes wrong."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#transferring-files",
    "href": "posts/chpc_wiki/index.html#transferring-files",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "Transferring Files",
    "text": "Transferring Files\nYou’ll need to move data between your local machine and the CHPC.\n\nDownloading from CHPC to Local Machine\n# Copy a single file\nscp username@scp.chpc.ac.za:/mnt/lustre/users/username/project/file.html ./\n\n# Copy a directory\nscp -r username@scp.chpc.ac.za:/mnt/lustre/users/username/project/results/ ./\n\n# Using rsync (better for large transfers, resumes if interrupted)\nrsync -avzP username@scp.chpc.ac.za:/mnt/lustre/users/username/project/results/ ./\nrsync flags: - -a: Archive mode (preserves permissions, timestamps) - -v: Verbose (shows progress) - -z: Compress during transfer - -P: Show progress and allow resume if interrupted\n\n\nUploading from Local Machine to CHPC\n# Copy a file\nscp file.txt username@scp.chpc.ac.za:/mnt/lustre/users/username/project/\n\n# Copy a directory\nscp -r my_data/ username@scp.chpc.ac.za:/mnt/lustre/users/username/project/\n\n# Using rsync\nrsync -avzP my_data/ username@scp.chpc.ac.za:/mnt/lustre/users/username/project/"
  },
  {
    "objectID": "posts/chpc_wiki/index.html#quick-reference-common-commands",
    "href": "posts/chpc_wiki/index.html#quick-reference-common-commands",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "Quick Reference: Common Commands",
    "text": "Quick Reference: Common Commands\nNavigation:\nwkd                                    # Go to working directory (if you set up alias)\ncd /mnt/lustre/users/username/         # Full path to working directory\ndu --si -s $HOME                       # Check disk usage\nInteractive sessions:\nqsub -I -l select=1:ncpus=4 -q serial -P PROJECT_CODE -l walltime=1:00:00\nscreen -S sessionname                  # Start screen session\nCtrl+A, D                              # Detach from screen\nscreen -r sessionname                  # Reattach to screen\nModules:\nmodule load chpc/BIOMODULES            # Load bioinformatics modules\nmodule avail 2&gt;&1 | grep tool_name     # Search for software\nmodule load tool_name/version          # Load a specific tool\nmodule list                            # Show loaded modules\nJobs:\nqsub script.sh                         # Submit job\nqstat -u username                      # Check your jobs\nqstat job_id                           # Check specific job\nqdel job_id                            # Cancel job\nFile transfer:\nscp file.txt username@scp.chpc.ac.za:/path/     # Upload file\nscp username@scp.chpc.ac.za:/path/file.txt ./   # Download file\nrsync -avzP local/ username@scp.chpc.ac.za:/path/  # Sync directory"
  },
  {
    "objectID": "posts/chpc_wiki/index.html#whats-next",
    "href": "posts/chpc_wiki/index.html#whats-next",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "What’s Next",
    "text": "What’s Next\nThis gets you up and running with the basics. Topics I’ll cover in future posts: - Using Singularity containers for reproducible environments - Setting up Conda for Python workflows - Parallelizing jobs across multiple nodes - Array jobs for running the same analysis on many samples - Best practices for managing large-scale genomics projects\nThe CHPC wiki is your best resource for diving deeper - now you have enough context to make sense of it."
  },
  {
    "objectID": "posts/chpc_wiki/index.html#acknowledgements",
    "href": "posts/chpc_wiki/index.html#acknowledgements",
    "title": "Getting started with the CHPC: a practical guide",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to the CHPC team for maintaining the infrastructure and documentation. Any errors or oversimplifications in this guide are mine, not theirs.\n\nFinal tip: Start small. Test your commands interactively, then write a script, then submit a short test job. Build complexity gradually. There’s a learning curve, but once you’re comfortable, the CHPC becomes an incredibly powerful research tool."
  },
  {
    "objectID": "posts/git_cheatsheet/index.html",
    "href": "posts/git_cheatsheet/index.html",
    "title": "Version control with Git: from zero to hero",
    "section": "",
    "text": "Here’s a scenario: You’re writing a script to process your data. It works. You decide to tweak something. Suddenly it doesn’t work. You can’t remember exactly what you changed. You have no way back to the working version except frantically pressing Ctrl+Z or digging through script_v2_final_ACTUAL_final.py.\nOr: You’re collaborating with someone on an analysis pipeline. They send you analysis.R. You make changes and send back analysis_edited.R. They make more changes and send analysis_edited_v2.R. A week later, neither of you knows which version has the correct parameters.\nVersion control solves this. Specifically, Git solves this.\nWhat Git gives you: - Snapshots of your work at any point in time - you can revert back if something breaks - Comments on changes - “fixed off-by-one error in loop” or “changed parameter to match paper methods” - Collaboration without chaos - multiple people can work on the same codebase without overwriting each other - Experimentation without risk - test new approaches on branches without touching your working code - A complete history - see exactly what changed, when, and why\nI use Git for this blog. I use it for my PhD analysis scripts. Once you get comfortable with it, you’ll wonder how you ever worked without it."
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#why-version-control-matters",
    "href": "posts/git_cheatsheet/index.html#why-version-control-matters",
    "title": "Version control with Git: from zero to hero",
    "section": "",
    "text": "Here’s a scenario: You’re writing a script to process your data. It works. You decide to tweak something. Suddenly it doesn’t work. You can’t remember exactly what you changed. You have no way back to the working version except frantically pressing Ctrl+Z or digging through script_v2_final_ACTUAL_final.py.\nOr: You’re collaborating with someone on an analysis pipeline. They send you analysis.R. You make changes and send back analysis_edited.R. They make more changes and send analysis_edited_v2.R. A week later, neither of you knows which version has the correct parameters.\nVersion control solves this. Specifically, Git solves this.\nWhat Git gives you: - Snapshots of your work at any point in time - you can revert back if something breaks - Comments on changes - “fixed off-by-one error in loop” or “changed parameter to match paper methods” - Collaboration without chaos - multiple people can work on the same codebase without overwriting each other - Experimentation without risk - test new approaches on branches without touching your working code - A complete history - see exactly what changed, when, and why\nI use Git for this blog. I use it for my PhD analysis scripts. Once you get comfortable with it, you’ll wonder how you ever worked without it."
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#git-vs.-github-theyre-not-the-same-thing",
    "href": "posts/git_cheatsheet/index.html#git-vs.-github-theyre-not-the-same-thing",
    "title": "Version control with Git: from zero to hero",
    "section": "Git vs. GitHub: They’re Not the Same Thing",
    "text": "Git vs. GitHub: They’re Not the Same Thing\nGit is version control software that runs on your local machine. It tracks changes to files in a repository (a project folder).\nGitHub is a website that hosts Git repositories online, making it easy to: - Back up your code in the cloud - Share code with collaborators - Contribute to open-source projects - Showcase your work\nYou can use Git without ever touching GitHub. GitHub is just one place to store remote repositories - GitLab, Bitbucket, and self-hosted servers are alternatives.\nThis guide focuses on Git itself. We’ll cover GitHub integration, but the core concepts work regardless of where you host your remote repositories."
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#installing-git",
    "href": "posts/git_cheatsheet/index.html#installing-git",
    "title": "Version control with Git: from zero to hero",
    "section": "Installing Git",
    "text": "Installing Git\nLinux (Ubuntu/Debian):\nsudo apt-get install git\nMac:\nbrew install git\nWindows: Download from git-scm.com or gitforwindows.org if you want a GUI and Unix interface. But I would not recommend this - if you’re setting up coding projects on Windows, refer to my previous post on WSL.\nOnce installed, verify:\ngit --version"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#first-time-setup",
    "href": "posts/git_cheatsheet/index.html#first-time-setup",
    "title": "Version control with Git: from zero to hero",
    "section": "First-Time Setup",
    "text": "First-Time Setup\nBefore you use Git, configure your identity. This information is attached to every commit you make:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\nUse the same email you’ll use for GitHub (if you plan to use it).\nSet your default text editor (optional but useful):\ngit config --global core.editor \"code\"  # VS Code for normal people\ngit config --global core.editor \"nano\"  # Nano if you love terminal\ngit config --global core.editor \"vim\"   # Vim for the hardcore \nFull list of editor options: Git config documentation\nCheck your configuration:\ngit config --list\nThis seems like a lot of setup, but you only need to do this once"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#your-first-repository",
    "href": "posts/git_cheatsheet/index.html#your-first-repository",
    "title": "Version control with Git: from zero to hero",
    "section": "Your First Repository",
    "text": "Your First Repository\nLet’s create a project and start tracking it.\nmkdir shopping\ncd shopping\nInitialize a Git repository:\ngit init\nThis creates a hidden .git directory where Git stores all its data. You can see it with:\nls -A\ncd .git\nls -A\nDon’t edit anything in here directly - Git manages it.\nNow create a file:\ntouch list.txt\nCheck the status:\ngit status\nGit knows the file exists, but it’s not tracking it yet. The file is “untracked.”"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#the-git-workflow-add-commit-repeat",
    "href": "posts/git_cheatsheet/index.html#the-git-workflow-add-commit-repeat",
    "title": "Version control with Git: from zero to hero",
    "section": "The Git Workflow: Add, Commit, Repeat",
    "text": "The Git Workflow: Add, Commit, Repeat\nGit has a three-stage workflow:\n\nWorking Directory - where you make changes\nStaging Area (Index) - where you prepare changes for saving\nRepository (.git directory) - where Git permanently stores committed changes\n\n\nStage a File\nTell Git to track the file:\ngit add list.txt\nCheck status again:\ngit status\nThe file is now “staged” - ready to be committed. Think of staging as putting items in a shopping basket before checkout.\n\n\nCommit Changes\nSave the staged changes to the repository:\ngit commit -m \"create shopping list\"\nThe -m flag adds a commit message describing what changed. Always write meaningful messages - “fixed bug” is useless; “fixed off-by-one error in read mapping loop” is helpful.\nCheck status:\ngit status\n“Nothing to commit, working tree clean” - Git has saved your snapshot.\n\n\nView History\nSee all commits:\ngit log\nYou’ll see the commit hash (a unique identifier), author, date, and message."
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#making-changes",
    "href": "posts/git_cheatsheet/index.html#making-changes",
    "title": "Version control with Git: from zero to hero",
    "section": "Making Changes",
    "text": "Making Changes\nEdit the file:\ncode list.txt  # Opens in VS Code\n# Add some items:\n# apples\n# bananas\n# milk\nCheck status:\ngit status\nGit knows the file changed, but hasn’t saved the changes yet.\nSee exactly what changed:\ngit diff\nThis shows line-by-line differences. Lines starting with - were removed; lines with + were added.\nStage and commit the changes:\ngit add list.txt\ngit commit -m \"add fruit and dairy\"\nCheck the log:\ngit log\ngit log -1  # Show only the most recent commit"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#understanding-git-architecture",
    "href": "posts/git_cheatsheet/index.html#understanding-git-architecture",
    "title": "Version control with Git: from zero to hero",
    "section": "Understanding Git Architecture",
    "text": "Understanding Git Architecture\nLet’s clarify what’s happening behind the scenes:\nWorking Directory: - Your project folder where you edit files - Files here are in the “modified” state if changed since the last commit\nStaging Area (Index): - Lives inside .git/ - A holding area for changes you want to commit - Use git add to move files here - Empties after each commit\n.git Directory: - The repository itself - Stores all commits, branches, history - Use git commit to save staged changes here permanently\nKey operations: - Checkout: Pull files from .git into your working directory (git checkout) - Staging: Prepare files for commit (git add) - Commit: Save staged files to .git permanently (git commit) - Push: Upload commits to a remote server like GitHub (git push) - Pull: Download commits from a remote server (git pull)\n\nWhy the Staging Area Exists\nWhy not just commit changes directly? The staging area lets you: - Review changes before committing - Commit only some modified files (not everything) - Build logical, atomic commits (one commit = one logical change)\nFor example, if you fix a bug and add a new feature in the same session, you can stage and commit them separately, making your history clearer."
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#comparing-versions",
    "href": "posts/git_cheatsheet/index.html#comparing-versions",
    "title": "Version control with Git: from zero to hero",
    "section": "Comparing Versions",
    "text": "Comparing Versions\nMake more changes to the file:\ncode list.txt\n# Add: ice-cream, yogurt, cheese\nStage the file:\ngit add list.txt\nTo see the difference between staged changes and the last commit:\ngit diff --staged\nThis shows what you’re about to commit.\nCommit the changes:\ngit commit -m \"added dairy products\""
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#the-head-pointer",
    "href": "posts/git_cheatsheet/index.html#the-head-pointer",
    "title": "Version control with Git: from zero to hero",
    "section": "The HEAD Pointer",
    "text": "The HEAD Pointer\nIn your log, you’ll see HEAD -&gt; main (or HEAD -&gt; master on older repos).\nHEAD is a pointer to your current location in the repository - usually the latest commit on the current branch. Think of it as “you are here” on a map.\nAs you commit, HEAD moves forward to the new commit. As you switch branches, HEAD moves to that branch’s latest commit."
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#going-back-in-time",
    "href": "posts/git_cheatsheet/index.html#going-back-in-time",
    "title": "Version control with Git: from zero to hero",
    "section": "Going Back in Time",
    "text": "Going Back in Time\nMade a mistake? Want to revert to an earlier version?\nCheck your history:\ngit log\nEach commit has a hash - a unique identifier like a3f2b1c.... You can also use relative references: - HEAD~1 = one commit before HEAD - HEAD~2 = two commits before HEAD - HEAD~3 = three commits before HEAD\n\nCheckout an Old Version\nRevert list.txt to three commits ago:\ngit checkout HEAD~3 list.txt\nYour file now contains the content from that commit. Check the file to verify.\nReturn to the latest version:\ngit checkout HEAD list.txt"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#undoing-mistakes",
    "href": "posts/git_cheatsheet/index.html#undoing-mistakes",
    "title": "Version control with Git: from zero to hero",
    "section": "Undoing Mistakes",
    "text": "Undoing Mistakes\nStaged a file by accident?\ngit reset list.txt\nThis unstages the file without changing your working copy.\nCommitted something you didn’t mean to?\nThree options for git reset, depending on how much you want to undo:\ngit reset HEAD~1 --soft\n\nRemoves the commit\nKeeps changes staged\nWorking copy unchanged\n\ngit reset HEAD~1 --mixed  # Default\n\nRemoves the commit\nUnstages changes\nWorking copy still contains changes\n\ngit reset HEAD~1 --hard\n\nRemoves the commit\nUnstages changes\nDeletes changes from working copy (destructive!)\n\nUse --soft or --mixed to undo commits while keeping your work. Only use --hard if you truly want to delete changes."
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#branches-parallel-development",
    "href": "posts/git_cheatsheet/index.html#branches-parallel-development",
    "title": "Version control with Git: from zero to hero",
    "section": "Branches: Parallel Development",
    "text": "Branches: Parallel Development\nBranches let you work on different versions of your project simultaneously without affecting the main codebase. This is critical for: - Testing experimental features - Working on a bug fix while keeping production code stable - Allowing multiple people to develop different features in parallel\n\nCreating and Switching Branches\nCreate a new branch:\ngit branch dairy\nSee all branches:\ngit branch\nThe * shows your current branch.\nSwitch to the new branch:\ngit checkout dairy\n# Or use the newer command:\ngit switch dairy\nMake changes on this branch:\ncode list.txt\n# Add: yogurt, cream, butter\ngit add list.txt\ngit commit -m \"expand dairy section\"\nThese changes only exist on the dairy branch. Switch back:\ngit switch main\nOpen list.txt - the dairy additions are gone because you’re back on the main branch.\n\n\nMerging Branches\nWhen you’re happy with changes on a branch, merge them back into main:\ngit switch main\ngit merge dairy\nGit combines the changes from dairy into main. If there are no conflicts, it creates a merge commit automatically.\nDelete the branch if you’re done with it:\ngit branch -d dairy\n\n\nWhen Merges Conflict\nIf two branches modify the same line, Git can’t automatically merge them. You’ll see:\nCONFLICT (content): Merge conflict in list.txt\nOpen the file. Git marks conflicts like this:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\napples\n=======\noranges\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; dairy\nEdit the file to resolve the conflict (keep one version, combine them, or write something new), remove the conflict markers, then:\ngit add list.txt\ngit commit -m \"resolve merge conflict\""
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#working-with-remote-repositories-github",
    "href": "posts/git_cheatsheet/index.html#working-with-remote-repositories-github",
    "title": "Version control with Git: from zero to hero",
    "section": "Working with Remote Repositories (GitHub)",
    "text": "Working with Remote Repositories (GitHub)\nSo far, everything has been local. To collaborate or back up your work, you need a remote repository.\n\nConnecting to GitHub\n\nCreate a repository on GitHub (don’t initialize with README or .gitignore)\nCopy the HTTPS URL, something like: https://github.com/username/repo-name.git\n\nAdd the remote:\ngit remote add origin https://github.com/username/repo-name.git\norigin is the conventional name for your primary remote repository. Check it worked:\ngit remote -v\nPush your local commits to GitHub:\ngit push -u origin main\nThe -u flag sets origin main as the default upstream, so future pushes can just be git push.\nIf you get an error about the branch name (main vs. master), rename your branch:\ngit branch -M main\n\n\nCloning an Existing Repository\nTo download someone else’s repository (or your own from another machine):\ngit clone https://github.com/username/repo-name.git\ncd repo-name\nThis creates a new directory with the full repository history.\n\n\nCollaboration Workflow\nWhen working with others:\n1. Pull before you work:\ngit pull origin main\nThis downloads new commits from GitHub.\n2. Make your changes, commit locally:\ngit add .\ngit commit -m \"add analysis script\"\n3. Push your commits:\ngit push origin main\nIf someone pushed commits while you were working, you’ll get an error. Pull first, resolve any conflicts, then push:\ngit pull origin main\ngit push origin main"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#feature-branch-workflow",
    "href": "posts/git_cheatsheet/index.html#feature-branch-workflow",
    "title": "Version control with Git: from zero to hero",
    "section": "Feature Branch Workflow",
    "text": "Feature Branch Workflow\nFor larger projects, never commit directly to main. Use feature branches:\nCreate a branch for your feature:\ngit branch feature/add-qc-plots\ngit switch feature/add-qc-plots\nMake changes and commit:\n# Edit files\ngit add src/qc_plots.R\ngit commit -m \"add quality control plotting functions\"\nPush the branch to GitHub:\ngit push -u origin feature/add-qc-plots\nOn GitHub, open a Pull Request: - Navigate to your repository - Click “Compare & pull request” - Describe your changes - Request review from collaborators - Once approved, merge into main\nAfter merging, update your local main branch:\ngit switch main\ngit pull origin main\nDelete the feature branch (optional):\ngit branch -d feature/add-qc-plots"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#what-are-hash-values-sha-1",
    "href": "posts/git_cheatsheet/index.html#what-are-hash-values-sha-1",
    "title": "Version control with Git: from zero to hero",
    "section": "What Are Hash Values (SHA-1)?",
    "text": "What Are Hash Values (SHA-1)?\nYou’ve seen those long alphanumeric strings in git log output - things like a3f2b1c4d5e6f7a8b9c0d1e2f3a4b5c6d7e8f9a0. These are hash values.\nWhat is a hash? A hash function takes input data (a file, a message, a commit) and produces a fixed-length output - the hash value. It’s like a fingerprint for data.\nProperties of hash functions: 1. Deterministic: Same input always produces the same hash 2. Fast to compute: Generating a hash is quick 3. Irreversible: You can’t reconstruct the original data from the hash 4. Unique (practically): Different inputs produce different hashes (collisions are astronomically rare) 5. Sensitive: Changing even one character changes the entire hash\nSHA-1 (Secure Hash Algorithm 1): - Produces a 160-bit (20-byte) hash value - Displayed as a 40-character hexadecimal string - Example: a94a8fe5ccb19ba61c4c0873d391e987982fbbd3\n\nWhy Git Uses Hashes\nEvery commit, file, and tree in Git is identified by its SHA-1 hash. This means:\n1. Integrity checking: Git can detect if any data has been corrupted. If a file changes even slightly, its hash changes, and Git knows.\n2. Unique identifiers: Each commit has a globally unique ID. No two commits will ever have the same hash (with overwhelming probability).\n3. Content-addressable storage: Git stores objects based on their content hash. If you commit the same file twice, Git stores it once because it has the same hash.\n4. Distributed development: When you clone a repository, you can verify you got exactly the same data by checking hashes.\n\n\nPractical Use of Hashes\nShort hashes: You don’t need the full 40 characters. Git accepts the first 7-10 characters:\ngit show a94a8fe     # Shows the commit\ngit reset --hard a94a8fe\ngit checkout a94a8fe script.py\nFinding specific commits:\ngit log --oneline   # Shows short hashes\ngit show a94a8fe    # Show details of a commit\nChecking integrity:\ngit fsck            # File system check - verifies hash integrity\n\n\nHash Collisions and Security\nIn theory, two different files could produce the same hash (a collision). In practice, with SHA-1’s 2^160 possible values, the probability is negligible for normal use.\nNote: SHA-1 has known cryptographic weaknesses. Git is transitioning to SHA-256 for better security, but SHA-1 is still the default and sufficient for version control purposes."
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#essential-commands-reference",
    "href": "posts/git_cheatsheet/index.html#essential-commands-reference",
    "title": "Version control with Git: from zero to hero",
    "section": "Essential Commands Reference",
    "text": "Essential Commands Reference\n\nSetup\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your@email.com\"\ngit config --list\n\n\nCreating Repositories\ngit init                    # Initialize new repository\ngit clone url              # Clone existing repository\n\n\nBasic Workflow\ngit status                 # Check what's changed\ngit add file.txt           # Stage specific file\ngit add .                  # Stage all changes\ngit commit -m \"message\"    # Commit staged changes\ngit log                    # View commit history\ngit log --oneline          # Condensed log\n\n\nViewing Changes\ngit diff                   # Changes in working directory\ngit diff --staged          # Changes in staging area\ngit show a3f2b1c          # Show specific commit\n\n\nUndoing Changes\ngit checkout HEAD file.txt    # Restore file to last commit\ngit reset file.txt            # Unstage file\ngit reset HEAD~1 --soft       # Undo last commit, keep changes staged\ngit reset HEAD~1 --mixed      # Undo last commit, unstage changes\ngit reset HEAD~1 --hard       # Undo last commit, delete changes\ngit revert a3f2b1c           # Create new commit undoing old commit\n\n\nBranches\ngit branch                 # List branches\ngit branch feature         # Create branch\ngit switch feature         # Switch to branch\ngit checkout feature       # Switch to branch (older syntax)\ngit merge feature          # Merge branch into current branch\ngit branch -d feature      # Delete branch\n\n\nRemote Repositories\ngit remote add origin url     # Add remote\ngit remote -v                 # View remotes\ngit push -u origin main       # Push and set upstream\ngit push                      # Push to upstream\ngit pull origin main          # Pull from remote\ngit fetch                     # Download remote changes without merging\n\n\nHelp\ngit --help                 # General help\ngit help command           # Help for specific command\ngit command --help         # Alternative help syntax"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#tips-and-best-practices",
    "href": "posts/git_cheatsheet/index.html#tips-and-best-practices",
    "title": "Version control with Git: from zero to hero",
    "section": "Tips and Best Practices",
    "text": "Tips and Best Practices\nCommit often: - Small, logical commits are easier to understand and revert - One commit = one logical change\nUse branches: - Keep main/master stable - Develop features on separate branches - Merge only when tested and working\nPull before you push: - Always git pull before starting work - Reduces merge conflicts\nDon’t commit secrets: - Never commit passwords, API keys, or credentials - Use .gitignore to exclude sensitive files\nCheck status frequently: - git status is your friend - Use it before and after staging/committing"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#what-to-learn-next",
    "href": "posts/git_cheatsheet/index.html#what-to-learn-next",
    "title": "Version control with Git: from zero to hero",
    "section": "What to Learn Next",
    "text": "What to Learn Next\nThis guide covers the fundamentals and some intermediate concepts. To go deeper:\nGit internals: Understand objects, trees, and how Git stores data\nAdvanced branching: Stashing, rebase, cherry-pick, interactive rebase\nGit hooks: Automate tasks on commit, push, etc.\nCollaboration workflows: Git Flow, GitHub Flow, trunk-based development\nSubmodules: Managing repositories within repositories\nGit LFS: Handling large files efficiently"
  },
  {
    "objectID": "posts/git_cheatsheet/index.html#resources",
    "href": "posts/git_cheatsheet/index.html#resources",
    "title": "Version control with Git: from zero to hero",
    "section": "Resources",
    "text": "Resources\nOfficial documentation: - Git documentation - GitHub Guides\nInteractive tutorials: - Learn Git Branching - visual, interactive - GitHub Learning Lab\nBooks: - Pro Git - comprehensive, free online\n\nNow go forth and commit…"
  }
]